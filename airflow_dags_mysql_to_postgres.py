import pandas as pd
import mysql.connector
import psycopg2
import logging
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ----------------------------
# 1Ô∏è‚É£ Database configs
# ----------------------------
# Use Docker network hostnames
MYSQL_HOST = "host.docker.internal"
MYSQL_PORT = 3306
MYSQL_USER = "root"
MYSQL_PASSWORD = "Whatsnew2711"
MYSQL_DB = 'coffee_shop_sales'

import os

PG_HOST = os.environ.get("POSTGRES_HOST", "postgres")  # Docker service name
PG_PORT = int(os.environ.get("POSTGRES_PORT", 5432))
PG_USER = os.environ.get("POSTGRES_USER", "postgres")  # Your PG admin user
PG_PASSWORD = os.environ.get("POSTGRES_PASSWORD", "Whatsnew2711")  # Your password
PG_DB = os.environ.get("POSTGRES_DB", "coffee_sales_data")  # Your actual DB

# ----------------------------
# 2Ô∏è‚É£ Helpers
# ----------------------------
def get_postgres_connection():
    try:
        return psycopg2.connect(
            host=PG_HOST, port=PG_PORT, user=PG_USER, password=PG_PASSWORD, dbname=PG_DB
        )
    except Exception as e:
        logging.error(f"‚ùå Failed to connect to PostgreSQL: {e}")
        return None

def get_latest_date_from_postgres():
    logging.info("üîç Checking latest date in PostgreSQL...")
    conn = get_postgres_connection()
    if not conn:
        return None
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT MAX(T2.transaction_date)
            FROM fact_sales AS T1
            JOIN dim_date AS T2
            ON T1.date_id = T2.date_id;
        """)
        latest_date = cur.fetchone()[0]
        cur.close()
        return latest_date
    except Exception as e:
        logging.error(f"‚ùå Failed to retrieve latest date: {e}")
        return None
    finally:
        conn.close()

# ----------------------------
# 3Ô∏è‚É£ ETL Functions
# ----------------------------
def extract_data_from_mysql(date_to_process):
    logging.info(f"üì• Extracting data from MySQL for {date_to_process}...")
    try:
        conn = mysql.connector.connect(
            host=MYSQL_HOST, port=MYSQL_PORT, user=MYSQL_USER,
            password=MYSQL_PASSWORD, database=MYSQL_DB
        )
        query = f"""
        SELECT
            transaction_id,
            transaction_date,
            transaction_time,
            transaction_qty,
            store_id,
            store_location,
            product_id,
            unit_price,
            product_category,
            product_type,
            product_detail
        FROM sales_data
        WHERE transaction_date = '{date_to_process}';
        """
        df = pd.read_sql(query, conn)
        return df
    except Exception as e:
        logging.error(f"‚ùå MySQL extraction failed: {e}")
        return None
    finally:
        if 'conn' in locals() and conn.is_connected():
            conn.close()
            logging.info("MySQL connection closed.")

def transform_data(df_raw):
    logging.info("üîÑ Transforming data for OLAP...")
    # Convert dates & times
    df_raw["transaction_date"] = pd.to_datetime(df_raw["transaction_date"], errors="coerce")
    df_raw["transaction_time"] = pd.to_datetime(df_raw["transaction_time"].astype(str), errors="coerce").dt.time
    df_raw = df_raw.dropna(subset=["transaction_date", "transaction_time"])
    # Calculate total_amount
    df_raw["total_amount"] = df_raw["transaction_qty"] * df_raw["unit_price"]

    # Dim tables
    dim_date = df_raw[["transaction_date"]].drop_duplicates().reset_index(drop=True)
    dim_date["day"] = dim_date["transaction_date"].dt.day
    dim_date["month"] = dim_date["transaction_date"].dt.month
    dim_date["year"] = dim_date["transaction_date"].dt.year
    dim_date["weekday"] = dim_date["transaction_date"].dt.day_name()

    dim_time = df_raw[["transaction_time"]].drop_duplicates().reset_index(drop=True)
    dim_time["hour"] = dim_time["transaction_time"].apply(lambda x: x.hour)
    dim_time["minute"] = dim_time["transaction_time"].apply(lambda x: x.minute)
    dim_time["second"] = dim_time["transaction_time"].apply(lambda x: x.second)

    dim_store = df_raw[["store_id", "store_location"]].drop_duplicates().reset_index(drop=True)
    dim_product = df_raw[["product_id","product_category","product_type","product_detail","unit_price"]].drop_duplicates().reset_index(drop=True)

    df_fact = df_raw.copy()
    return dim_date, dim_time, dim_store, dim_product, df_fact

def load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact):
    logging.info("üì§ Loading data to PostgreSQL...")
    conn = get_postgres_connection()
    if not conn:
        return
    try:
        cur = conn.cursor()
        # Idempotent insert logic simplified (omitted for brevity)
        # You can reuse your previous "check then insert" loops here
        conn.commit()
        cur.close()
        logging.info("‚úÖ Data loaded successfully!")
    except Exception as e:
        logging.error(f"‚ùå Load to PostgreSQL failed: {e}")
        conn.rollback()
    finally:
        conn.close()

def run_daily_etl():
    latest_date = get_latest_date_from_postgres()
    if latest_date:
        date_to_process = (latest_date + timedelta(days=1)).strftime('%Y-%m-%d')
    else:
        date_to_process = '2025-01-01'
    logging.info(f"üìÜ Processing date: {date_to_process}")
    df_raw = extract_data_from_mysql(date_to_process)
    if df_raw is not None and not df_raw.empty:
        dim_date, dim_time, dim_store, dim_product, df_fact = transform_data(df_raw)
        load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact)
        logging.info(f"‚úÖ Completed ETL for {date_to_process}")
    else:
        logging.warning(f"‚ö†Ô∏è No data for {date_to_process}. Skipping.")

# ----------------------------
# 4Ô∏è‚É£ DAG Definition
# ----------------------------
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023,1,1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    dag_id='daily_backdate_mysql_to_postgres',
    default_args=default_args,
    description='Daily incremental ETL from MySQL to PostgreSQL',
    schedule='*/5 * * * *',
    catchup=False
) as dag:
    daily_task = PythonOperator(
        task_id='run_daily_incremental',
        python_callable=run_daily_etl
    )

daily_task