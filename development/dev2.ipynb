{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8c9ea5-1c1e-4073-afed-52d939b9c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter start date (YYYY-MM-DD):  2025-01-01\n",
      "Enter end date (YYYY-MM-DD):  2025-01-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Extracting data from 'Coffee Shop Sales2.csv'...\n",
      "🔄 Transforming data...\n",
      "🚀 Loading data to MySQL...\n",
      "Successfully connected to MySQL database.\n",
      "✅ Load complete! Inserted/updated 2742 records.\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "# scheduled_oltp_update.py\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import logging\n",
    "\n",
    "# Set up logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_data(input_csv):\n",
    "    \"\"\"\n",
    "    Extracts data from a specified CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"📥 Extracting data from '{input_csv}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, sep=\";\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The file '{input_csv}' was not found.\")\n",
    "        return None\n",
    "\n",
    "def transform_data(df, start_offset_date, end_offset_date):\n",
    "    \"\"\"\n",
    "    Transforms the raw DataFrame by:\n",
    "    - Filtering by date range\n",
    "    - Formatting dates & times\n",
    "    - Mapping store locations\n",
    "    - Converting currency\n",
    "    - Regenerating unique transaction IDs\n",
    "    \"\"\"\n",
    "    print(\"🔄 Transforming data...\")\n",
    "\n",
    "    # Convert date and time\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "    df['transaction_time'] = pd.to_datetime(\n",
    "        df['transaction_time'], format='%H:%M:%S', errors='coerce'\n",
    "    ).dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Filter by date range\n",
    "    start_date = pd.to_datetime(start_offset_date)\n",
    "    end_date = pd.to_datetime(end_offset_date)\n",
    "    df_filtered = df[\n",
    "        (df['transaction_date'] >= start_date) & (df['transaction_date'] <= end_date)\n",
    "    ].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"⚠ No data found for the range {start_offset_date} to {end_offset_date}.\")\n",
    "        return df_filtered\n",
    "\n",
    "    # Format date for output\n",
    "    df_filtered['transaction_date'] = df_filtered['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Map store locations\n",
    "    location_map = {\n",
    "        \"Hell's Kitchen\": \"East Campus\",\n",
    "        \"Astoria\": \"West Campus\",\n",
    "        \"Lower Manhattan\": \"Main Campus\"\n",
    "    }\n",
    "    df_filtered['store_location'] = df_filtered['store_location'].replace(location_map)\n",
    "\n",
    "    # Convert unit_price\n",
    "    df_filtered['unit_price'] = (df_filtered['unit_price'] * 15).round(2)\n",
    "\n",
    "    # Convert key columns to string safely\n",
    "    cols_to_convert = ['transaction_id', 'store_id', 'product_id']\n",
    "    df_filtered[cols_to_convert] = df_filtered[cols_to_convert].apply(\n",
    "        lambda col: col.dropna().astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # Force unique transaction IDs by appending the date\n",
    "    df_filtered['transaction_id'] = (\n",
    "        df_filtered['transaction_id'].astype(str) + \"_\" +\n",
    "        pd.to_datetime(df_filtered['transaction_date']).dt.strftime('%Y%m%d')\n",
    "    )\n",
    "    \n",
    "    # Reorder columns to match the MySQL table structure\n",
    "    df_transformed = df_filtered.reindex(columns=[\n",
    "        'transaction_id', 'transaction_date', 'transaction_time', 'transaction_qty',\n",
    "        'store_id', 'store_location', 'product_id', 'unit_price',\n",
    "        'product_category', 'product_type', 'product_detail'\n",
    "    ], fill_value=None)\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def load_data_to_mysql(df, db_config, table_name):\n",
    "    \"\"\"\n",
    "    Loads transformed data into a MySQL table.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"⚠ No data to load to MySQL.\")\n",
    "        return\n",
    "\n",
    "    print(\"🚀 Loading data to MySQL...\")\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Successfully connected to MySQL database.\")\n",
    "\n",
    "        # Prepare the list of tuples for insertion\n",
    "        records = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "        # The INSERT query with ON DUPLICATE KEY UPDATE to handle idempotency\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} (\n",
    "                transaction_id, transaction_date, transaction_time, transaction_qty,\n",
    "                store_id, store_location, product_id, unit_price,\n",
    "                product_category, product_type, product_detail\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                transaction_date=VALUES(transaction_date),\n",
    "                transaction_time=VALUES(transaction_time),\n",
    "                transaction_qty=VALUES(transaction_qty),\n",
    "                store_id=VALUES(store_id),\n",
    "                store_location=VALUES(store_location),\n",
    "                product_id=VALUES(product_id),\n",
    "                unit_price=VALUES(unit_price),\n",
    "                product_category=VALUES(product_category),\n",
    "                product_type=VALUES(product_type),\n",
    "                product_detail=VALUES(product_detail)\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.executemany(insert_query, records)\n",
    "        conn.commit()\n",
    "        print(f\"✅ Load complete! Inserted/updated {cursor.rowcount} records.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "        print(\"MySQL connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"Coffee Shop Sales2.csv\"\n",
    "    table_name = \"sales_data\"\n",
    "    \n",
    "    db_config = {\n",
    "        'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': 'Whatsnew2711',\n",
    "        'database': 'coffee_shop_sales'\n",
    "    }\n",
    "\n",
    "    start_date = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
    "    end_date = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
    "\n",
    "    df_raw = extract_data(input_file)\n",
    "    if df_raw is not None:\n",
    "        df_transformed = transform_data(df_raw, start_date, end_date)\n",
    "        if df_transformed is not None:\n",
    "            load_data_to_mysql(df_transformed, db_config, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2df0ef9-5f9b-4e63-b761-38d5725af18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter start date (YYYY-MM-DD):  2025-01-01\n",
      "Enter end date (YYYY-MM-DD):  2025-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:⏳ Starting backdating process from 2025-01-01 to 2025-01-01...\n",
      "INFO:root:📆 Processing date: 2025-01-01\n",
      "INFO:root:📥 Extracting data from MySQL for 2025-01-01...\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_13876\\2017162868.py:79: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_raw = pd.read_sql(query, conn)\n",
      "INFO:root:✅ Extracted 549 records.\n",
      "INFO:root:MySQL connection closed.\n",
      "INFO:root:🔄 Transforming data for OLAP...\n",
      "INFO:root:✅ Transformation completed. Records: 549\n",
      "INFO:root:📤 Loading data to PostgreSQL...\n",
      "INFO:root:✅ Data loaded to PostgreSQL successfully!\n",
      "INFO:root:✅ Completed ETL for: 2025-01-01\n",
      "INFO:root:🎉 Backdating process finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up logging for visibility and easy debugging.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Database Connection Configurations\n",
    "# =========================================================\n",
    "# Configuration for the MySQL OLTP database.\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"Whatsnew2711\"\n",
    "MYSQL_DB = 'coffee_shop_sales'\n",
    "\n",
    "# Configuration for the PostgreSQL OLAP database.\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"Whatsnew2711\"\n",
    "PG_DB = \"coffee_sales_data\"\n",
    "\n",
    "def get_postgres_connection():\n",
    "    \"\"\"Establishes and returns a PostgreSQL database connection.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD,\n",
    "            dbname=PG_DB\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ ETL Functions\n",
    "# =========================================================\n",
    "\n",
    "def extract_data_from_mysql(date_to_process):\n",
    "    \"\"\"\n",
    "    Extracts daily transaction data from MySQL OLTP database\n",
    "    for a specific date.\n",
    "    \"\"\"\n",
    "    logging.info(f\"📥 Extracting data from MySQL for {date_to_process}...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            port=MYSQL_PORT,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            transaction_date,\n",
    "            transaction_time,\n",
    "            transaction_qty,\n",
    "            store_id,\n",
    "            store_location,\n",
    "            product_id,\n",
    "            unit_price,\n",
    "            product_category,\n",
    "            product_type,\n",
    "            product_detail\n",
    "        FROM sales_data\n",
    "        WHERE transaction_date = '{date_to_process}';\n",
    "        \"\"\"\n",
    "        df_raw = pd.read_sql(query, conn)\n",
    "        logging.info(f\"✅ Extracted {len(df_raw)} records.\")\n",
    "        return df_raw\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"MySQL connection closed.\")\n",
    "\n",
    "def transform_data(df_raw):\n",
    "    \"\"\"\n",
    "    Transforms raw MySQL data into star schema DataFrames that match PostgreSQL tables.\n",
    "    Returns: dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "    \"\"\"\n",
    "    logging.info(\"🔄 Transforming data for OLAP...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_date to datetime\n",
    "    # ----------------------------\n",
    "    if \"transaction_date\" in df_raw.columns:\n",
    "        df_raw[\"transaction_date\"] = pd.to_datetime(df_raw[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_time from timedelta to time\n",
    "    # ----------------------------\n",
    "    if \"transaction_time\" in df_raw.columns:\n",
    "        if pd.api.types.is_timedelta64_dtype(df_raw[\"transaction_time\"]):\n",
    "            df_raw[\"transaction_time\"] = df_raw[\"transaction_time\"].apply(\n",
    "                lambda x: (pd.Timestamp(\"00:00:00\") + x).time() if pd.notnull(x) else None\n",
    "            )\n",
    "        else:\n",
    "            df_raw[\"transaction_time\"] = pd.to_datetime(\n",
    "                df_raw[\"transaction_time\"].astype(str), errors=\"coerce\"\n",
    "            ).dt.time\n",
    "\n",
    "    # Drop rows where conversion failed\n",
    "    df_raw = df_raw.dropna(subset=[\"transaction_date\", \"transaction_time\"])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Calculate total_amount\n",
    "    # ----------------------------\n",
    "    df_raw[\"total_amount\"] = df_raw[\"transaction_qty\"] * df_raw[\"unit_price\"]\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Date\n",
    "    # ----------------------------\n",
    "    dim_date = df_raw[[\"transaction_date\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_date[\"day\"] = dim_date[\"transaction_date\"].dt.day\n",
    "    dim_date[\"month\"] = dim_date[\"transaction_date\"].dt.month\n",
    "    dim_date[\"year\"] = dim_date[\"transaction_date\"].dt.year\n",
    "    dim_date[\"weekday\"] = dim_date[\"transaction_date\"].dt.day_name()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Time\n",
    "    # ----------------------------\n",
    "    dim_time = df_raw[[\"transaction_time\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_time[\"hour\"] = dim_time[\"transaction_time\"].apply(lambda x: x.hour)\n",
    "    dim_time[\"minute\"] = dim_time[\"transaction_time\"].apply(lambda x: x.minute)\n",
    "    dim_time[\"second\"] = dim_time[\"transaction_time\"].apply(lambda x: x.second)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Store\n",
    "    # ----------------------------\n",
    "    dim_store = df_raw[[\"store_id\", \"store_location\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Product\n",
    "    # ----------------------------\n",
    "    dim_product = df_raw[[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Fact Table\n",
    "    # ----------------------------\n",
    "    df_fact = df_raw.copy()\n",
    "    df_fact = df_fact.merge(dim_date, on=\"transaction_date\")\n",
    "    df_fact = df_fact.merge(dim_time, on=\"transaction_time\")\n",
    "    df_fact = df_fact.merge(dim_store, on=[\"store_id\", \"store_location\"])\n",
    "    df_fact = df_fact.merge(dim_product, on=[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"])\n",
    "\n",
    "    df_fact = df_fact[[\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",  # maps to dim_date.date_id in load\n",
    "        \"transaction_time\",  # maps to dim_time.time_id\n",
    "        \"store_id\",          # maps to dim_store.store_id\n",
    "        \"product_id\",        # maps to dim_product.product_id\n",
    "        \"transaction_qty\",\n",
    "        \"total_amount\"\n",
    "    ]]\n",
    "\n",
    "    logging.info(f\"✅ Transformation completed. Records: {len(df_fact)}\")\n",
    "    return dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "\n",
    "def load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact):\n",
    "    \"\"\"\n",
    "    Loads transformed data into PostgreSQL OLAP database\n",
    "    by checking for existence before inserting to avoid duplicates.\n",
    "    \"\"\"\n",
    "    logging.info(\"📤 Loading data to PostgreSQL...\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_postgres_connection()\n",
    "        if not conn:\n",
    "            return\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Date (Check then Insert)\n",
    "        # -----------------------------\n",
    "        date_id_map = {}\n",
    "        for _, row in dim_date.iterrows():\n",
    "            cur.execute(\"SELECT date_id FROM dim_date WHERE transaction_date = %s;\", (row[\"transaction_date\"].date(),))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                date_id_map[row[\"transaction_date\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_date (transaction_date, day, month, year, weekday)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                    RETURNING date_id;\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_date\"].date(),\n",
    "                    int(row[\"day\"]),\n",
    "                    int(row[\"month\"]),\n",
    "                    int(row[\"year\"]),\n",
    "                    row[\"weekday\"]\n",
    "                ))\n",
    "                date_id_map[row[\"transaction_date\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Time (Check then Insert)\n",
    "        # -----------------------------\n",
    "        time_id_map = {}\n",
    "        for _, row in dim_time.iterrows():\n",
    "            cur.execute(\"SELECT time_id FROM dim_time WHERE transaction_time = %s;\", (row[\"transaction_time\"],))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                time_id_map[row[\"transaction_time\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_time (transaction_time, hour, minute, second)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING time_id;\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_time\"],\n",
    "                    int(row[\"hour\"]),\n",
    "                    int(row[\"minute\"]),\n",
    "                    int(row[\"second\"])\n",
    "                ))\n",
    "                time_id_map[row[\"transaction_time\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Store (Check then Insert)\n",
    "        # -----------------------------\n",
    "        store_id_map = {}\n",
    "        for _, row in dim_store.iterrows():\n",
    "            cur.execute(\"SELECT store_id FROM dim_store WHERE store_location = %s;\", (row[\"store_location\"],))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                store_id_map[row[\"store_id\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_store (store_location)\n",
    "                    VALUES (%s)\n",
    "                    RETURNING store_id;\n",
    "                \"\"\", (row[\"store_location\"],))\n",
    "                store_id_map[row[\"store_id\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Product (Check then Insert)\n",
    "        # -----------------------------\n",
    "        product_id_map = {}\n",
    "        for _, row in dim_product.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT product_id FROM dim_product\n",
    "                WHERE product_category = %s AND product_type = %s AND product_detail = %s;\n",
    "            \"\"\", (row[\"product_category\"], row[\"product_type\"], row[\"product_detail\"]))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                product_id_map[row[\"product_id\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_product (product_category, product_type, product_detail, unit_price)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING product_id;\n",
    "                \"\"\", (\n",
    "                    row[\"product_category\"],\n",
    "                    row[\"product_type\"],\n",
    "                    row[\"product_detail\"],\n",
    "                    float(row[\"unit_price\"])\n",
    "                ))\n",
    "                product_id_map[row[\"product_id\"]] = cur.fetchone()[0]\n",
    "        \n",
    "        conn.commit()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fact Table (Check then Insert)\n",
    "        # -----------------------------\n",
    "        for _, row in df_fact.iterrows():\n",
    "            cur.execute(\"SELECT transaction_id FROM fact_sales WHERE transaction_id = %s;\", (row[\"transaction_id\"],))\n",
    "            result = cur.fetchone()\n",
    "            \n",
    "            if not result:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO fact_sales (transaction_id, date_id, time_id, store_id, product_id, transaction_qty, total_amount)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_id\"],\n",
    "                    date_id_map[row[\"transaction_date\"]],\n",
    "                    time_id_map[row[\"transaction_time\"]],\n",
    "                    store_id_map[row[\"store_id\"]],\n",
    "                    product_id_map[row[\"product_id\"]],\n",
    "                    int(row[\"transaction_qty\"]),\n",
    "                    float(row[\"total_amount\"])\n",
    "                ))\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        logging.info(\"✅ Data loaded to PostgreSQL successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to load data to PostgreSQL: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ Main Execution Logic for Backdating\n",
    "# =========================================================\n",
    "\n",
    "def run_backdate(start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    Runs the full ETL pipeline for a specified date range.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert string dates to datetime.date objects for easier iteration\n",
    "        start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "    except ValueError:\n",
    "        logging.error(\"❌ Invalid date format. Please use YYYY-MM-DD.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"⏳ Starting backdating process from {start_date} to {end_date}...\")\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_to_process = current_date.strftime('%Y-%m-%d')\n",
    "        logging.info(f\"📆 Processing date: {date_to_process}\")\n",
    "\n",
    "        # --- Extract ---\n",
    "        df_raw = extract_data_from_mysql(date_to_process)\n",
    "        \n",
    "        # Check if extraction was successful and data exists\n",
    "        if df_raw is not None and not df_raw.empty:\n",
    "            # --- Transform ---\n",
    "            try:\n",
    "                dim_date, dim_time, dim_store, dim_product, df_fact = transform_data(df_raw)\n",
    "                # --- Load ---\n",
    "                load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact)\n",
    "                logging.info(f\"✅ Completed ETL for: {date_to_process}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"❌ ETL process failed for {date_to_process}: {e}\")\n",
    "        else:\n",
    "            logging.warning(f\"⚠️ No data found for {date_to_process} or extraction failed. Skipping.\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    logging.info(\"🎉 Backdating process finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_input = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
    "    end_input = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
    "    run_backdate(start_input, end_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c203530e-b0bf-4c6e-b510-edeed911caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 19:38:35,596 - INFO - 🔍 Checking for the latest date in PostgreSQL...\n",
      "2025-08-20 19:38:35,739 - INFO - ✅ Latest date found in database: 2025-01-02\n",
      "2025-08-20 19:38:35,743 - INFO - 📆 Processing date: 2025-01-03\n",
      "2025-08-20 19:38:35,744 - INFO - 📥 Extracting data from MySQL for 2025-01-03...\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_2500\\880047466.py:79: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_raw = pd.read_sql(query, conn)\n",
      "2025-08-20 19:38:36,513 - INFO - ✅ Extracted 582 records.\n",
      "2025-08-20 19:38:36,520 - INFO - MySQL connection closed.\n",
      "2025-08-20 19:38:36,523 - INFO - 🔄 Transforming data for OLAP...\n",
      "2025-08-20 19:38:36,703 - INFO - ✅ Transformation completed. Records: 582\n",
      "2025-08-20 19:38:36,707 - INFO - 📤 Loading data to PostgreSQL...\n",
      "2025-08-20 19:38:38,259 - INFO - ✅ Data loaded to PostgreSQL successfully!\n",
      "2025-08-20 19:38:38,264 - INFO - ✅ Completed ETL for: 2025-01-03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up logging for visibility and easy debugging.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Database Connection Configurations\n",
    "# =========================================================\n",
    "# Configuration for the MySQL OLTP database.\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"Whatsnew2711\"\n",
    "MYSQL_DB = 'coffee_shop_sales'\n",
    "\n",
    "# Configuration for the PostgreSQL OLAP database.\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"Whatsnew2711\"\n",
    "PG_DB = \"coffee_sales_data\"\n",
    "\n",
    "def get_postgres_connection():\n",
    "    \"\"\"Establishes and returns a PostgreSQL database connection.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD,\n",
    "            dbname=PG_DB\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ ETL Functions\n",
    "# =========================================================\n",
    "\n",
    "def extract_data_from_mysql(date_to_process):\n",
    "    \"\"\"\n",
    "    Extracts daily transaction data from MySQL OLTP database\n",
    "    for a specific date.\n",
    "    \"\"\"\n",
    "    logging.info(f\"📥 Extracting data from MySQL for {date_to_process}...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            port=MYSQL_PORT,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            transaction_date,\n",
    "            transaction_time,\n",
    "            transaction_qty,\n",
    "            store_id,\n",
    "            store_location,\n",
    "            product_id,\n",
    "            unit_price,\n",
    "            product_category,\n",
    "            product_type,\n",
    "            product_detail\n",
    "        FROM sales_data\n",
    "        WHERE transaction_date = '{date_to_process}';\n",
    "        \"\"\"\n",
    "        df_raw = pd.read_sql(query, conn)\n",
    "        logging.info(f\"✅ Extracted {len(df_raw)} records.\")\n",
    "        return df_raw\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"MySQL connection closed.\")\n",
    "\n",
    "def transform_data(df_raw):\n",
    "    \"\"\"\n",
    "    Transforms raw MySQL data into star schema DataFrames that match PostgreSQL tables.\n",
    "    Returns: dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "    \"\"\"\n",
    "    logging.info(\"🔄 Transforming data for OLAP...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_date to datetime\n",
    "    # ----------------------------\n",
    "    if \"transaction_date\" in df_raw.columns:\n",
    "        df_raw[\"transaction_date\"] = pd.to_datetime(df_raw[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_time from timedelta to time\n",
    "    # ----------------------------\n",
    "    if \"transaction_time\" in df_raw.columns:\n",
    "        if pd.api.types.is_timedelta64_dtype(df_raw[\"transaction_time\"]):\n",
    "            df_raw[\"transaction_time\"] = df_raw[\"transaction_time\"].apply(\n",
    "                lambda x: (pd.Timestamp(\"00:00:00\") + x).time() if pd.notnull(x) else None\n",
    "            )\n",
    "        else:\n",
    "            df_raw[\"transaction_time\"] = pd.to_datetime(\n",
    "                df_raw[\"transaction_time\"].astype(str), errors=\"coerce\"\n",
    "            ).dt.time\n",
    "\n",
    "    # Drop rows where conversion failed\n",
    "    df_raw = df_raw.dropna(subset=[\"transaction_date\", \"transaction_time\"])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Calculate total_amount\n",
    "    # ----------------------------\n",
    "    df_raw[\"total_amount\"] = df_raw[\"transaction_qty\"] * df_raw[\"unit_price\"]\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Date\n",
    "    # ----------------------------\n",
    "    dim_date = df_raw[[\"transaction_date\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_date[\"day\"] = dim_date[\"transaction_date\"].dt.day\n",
    "    dim_date[\"month\"] = dim_date[\"transaction_date\"].dt.month\n",
    "    dim_date[\"year\"] = dim_date[\"transaction_date\"].dt.year\n",
    "    dim_date[\"weekday\"] = dim_date[\"transaction_date\"].dt.day_name()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Time\n",
    "    # ----------------------------\n",
    "    dim_time = df_raw[[\"transaction_time\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_time[\"hour\"] = dim_time[\"transaction_time\"].apply(lambda x: x.hour)\n",
    "    dim_time[\"minute\"] = dim_time[\"transaction_time\"].apply(lambda x: x.minute)\n",
    "    dim_time[\"second\"] = dim_time[\"transaction_time\"].apply(lambda x: x.second)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Store\n",
    "    # ----------------------------\n",
    "    dim_store = df_raw[[\"store_id\", \"store_location\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Product\n",
    "    # ----------------------------\n",
    "    dim_product = df_raw[[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Fact Table\n",
    "    # ----------------------------\n",
    "    df_fact = df_raw.copy()\n",
    "    df_fact = df_fact.merge(dim_date, on=\"transaction_date\")\n",
    "    df_fact = df_fact.merge(dim_time, on=\"transaction_time\")\n",
    "    df_fact = df_fact.merge(dim_store, on=[\"store_id\", \"store_location\"])\n",
    "    df_fact = df_fact.merge(dim_product, on=[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"])\n",
    "\n",
    "    df_fact = df_fact[[\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",  # maps to dim_date.date_id in load\n",
    "        \"transaction_time\",  # maps to dim_time.time_id\n",
    "        \"store_id\",          # maps to dim_store.store_id\n",
    "        \"product_id\",        # maps to dim_product.product_id\n",
    "        \"transaction_qty\",\n",
    "        \"total_amount\"\n",
    "    ]]\n",
    "\n",
    "    logging.info(f\"✅ Transformation completed. Records: {len(df_fact)}\")\n",
    "    return dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "\n",
    "def load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact):\n",
    "    \"\"\"\n",
    "    Loads transformed data into PostgreSQL OLAP database\n",
    "    by checking for existence before inserting to avoid duplicates.\n",
    "    \"\"\"\n",
    "    logging.info(\"📤 Loading data to PostgreSQL...\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_postgres_connection()\n",
    "        if not conn:\n",
    "            return\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Date (Check then Insert)\n",
    "        # -----------------------------\n",
    "        date_id_map = {}\n",
    "        for _, row in dim_date.iterrows():\n",
    "            cur.execute(\"SELECT date_id FROM dim_date WHERE transaction_date = %s;\", (row[\"transaction_date\"].date(),))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                date_id_map[row[\"transaction_date\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_date (transaction_date, day, month, year, weekday)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                    RETURNING date_id;\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_date\"].date(),\n",
    "                    int(row[\"day\"]),\n",
    "                    int(row[\"month\"]),\n",
    "                    int(row[\"year\"]),\n",
    "                    row[\"weekday\"]\n",
    "                ))\n",
    "                date_id_map[row[\"transaction_date\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Time (Check then Insert)\n",
    "        # -----------------------------\n",
    "        time_id_map = {}\n",
    "        for _, row in dim_time.iterrows():\n",
    "            cur.execute(\"SELECT time_id FROM dim_time WHERE transaction_time = %s;\", (row[\"transaction_time\"],))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                time_id_map[row[\"transaction_time\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_time (transaction_time, hour, minute, second)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING time_id;\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_time\"],\n",
    "                    int(row[\"hour\"]),\n",
    "                    int(row[\"minute\"]),\n",
    "                    int(row[\"second\"])\n",
    "                ))\n",
    "                time_id_map[row[\"transaction_time\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Store (Check then Insert)\n",
    "        # -----------------------------\n",
    "        store_id_map = {}\n",
    "        for _, row in dim_store.iterrows():\n",
    "            cur.execute(\"SELECT store_id FROM dim_store WHERE store_location = %s;\", (row[\"store_location\"],))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                store_id_map[row[\"store_id\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_store (store_location)\n",
    "                    VALUES (%s)\n",
    "                    RETURNING store_id;\n",
    "                \"\"\", (row[\"store_location\"],))\n",
    "                store_id_map[row[\"store_id\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Product (Check then Insert)\n",
    "        # -----------------------------\n",
    "        product_id_map = {}\n",
    "        for _, row in dim_product.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT product_id FROM dim_product\n",
    "                WHERE product_category = %s AND product_type = %s AND product_detail = %s;\n",
    "            \"\"\", (row[\"product_category\"], row[\"product_type\"], row[\"product_detail\"]))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                product_id_map[row[\"product_id\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_product (product_category, product_type, product_detail, unit_price)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING product_id;\n",
    "                \"\"\", (\n",
    "                    row[\"product_category\"],\n",
    "                    row[\"product_type\"],\n",
    "                    row[\"product_detail\"],\n",
    "                    float(row[\"unit_price\"])\n",
    "                ))\n",
    "                product_id_map[row[\"product_id\"]] = cur.fetchone()[0]\n",
    "        \n",
    "        conn.commit()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fact Table (Check then Insert)\n",
    "        # -----------------------------\n",
    "        for _, row in df_fact.iterrows():\n",
    "            cur.execute(\"SELECT transaction_id FROM fact_sales WHERE transaction_id = %s;\", (row[\"transaction_id\"],))\n",
    "            result = cur.fetchone()\n",
    "            \n",
    "            if not result:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO fact_sales (transaction_id, date_id, time_id, store_id, product_id, transaction_qty, total_amount)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_id\"],\n",
    "                    date_id_map[row[\"transaction_date\"]],\n",
    "                    time_id_map[row[\"transaction_time\"]],\n",
    "                    store_id_map[row[\"store_id\"]],\n",
    "                    product_id_map[row[\"product_id\"]],\n",
    "                    int(row[\"transaction_qty\"]),\n",
    "                    float(row[\"total_amount\"])\n",
    "                ))\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        logging.info(\"✅ Data loaded to PostgreSQL successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to load data to PostgreSQL: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ Main Execution Logic for Backdating\n",
    "# =========================================================\n",
    "\n",
    "def get_latest_date_from_postgres():\n",
    "    \"\"\"\n",
    "    Queries PostgreSQL to find the most recent transaction date in the\n",
    "    fact_sales table.\n",
    "    \"\"\"\n",
    "    logging.info(\"🔍 Checking for the latest date in PostgreSQL...\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_postgres_connection()\n",
    "        if not conn:\n",
    "            return None\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "        # Join fact_sales with dim_date to get the actual date\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MAX(T2.transaction_date)\n",
    "            FROM fact_sales AS T1\n",
    "            JOIN dim_date AS T2\n",
    "            ON T1.date_id = T2.date_id;\n",
    "        \"\"\")\n",
    "        latest_date = cur.fetchone()[0]\n",
    "        cur.close()\n",
    "\n",
    "        if latest_date:\n",
    "            logging.info(f\"✅ Latest date found in database: {latest_date}\")\n",
    "            return latest_date\n",
    "        else:\n",
    "            logging.warning(\"⚠️ No data found in fact_sales table. Starting from default date.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to retrieve latest date from PostgreSQL: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def run_daily_etl():\n",
    "    \"\"\"\n",
    "    Runs the ETL pipeline for a single day, determined by the latest\n",
    "    data in the PostgreSQL database.\n",
    "    \"\"\"\n",
    "    # Find the latest date in the OLAP database\n",
    "    latest_date_in_db = get_latest_date_from_postgres()\n",
    "    \n",
    "    # Determine the date to process. If no data exists, start from the beginning.\n",
    "    if latest_date_in_db:\n",
    "        # Load the next day's data\n",
    "        date_to_process = (latest_date_in_db + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        # Default start date for the backfilling.\n",
    "        date_to_process = '2025-01-01'\n",
    "\n",
    "    logging.info(f\"📆 Processing date: {date_to_process}\")\n",
    "\n",
    "    # --- Extract ---\n",
    "    df_raw = extract_data_from_mysql(date_to_process)\n",
    "    \n",
    "    # Check if extraction was successful and data exists\n",
    "    if df_raw is not None and not df_raw.empty:\n",
    "        # --- Transform ---\n",
    "        try:\n",
    "            dim_date, dim_time, dim_store, dim_product, df_fact = transform_data(df_raw)\n",
    "            # --- Load ---\n",
    "            load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact)\n",
    "            logging.info(f\"✅ Completed ETL for: {date_to_process}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ ETL process failed for {date_to_process}: {e}\")\n",
    "    else:\n",
    "        logging.warning(f\"⚠️ No data found for {date_to_process} or extraction failed. Skipping.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_daily_etl()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718b2e44-8f3e-4ae4-8a19-4c27be4d9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv(\"AIRFLOW_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21125cf0-1b6a-49dd-b8d0-3b7a74437a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from etl_oltp import extract_data, transform_data, load_data\n",
    "\n",
    "INPUT_FILE = \"Coffee Shop Sales2.csv\"\n",
    "OUTPUT_FILE = \"transformed.csv\"\n",
    "\n",
    "def get_next_date_to_process(output_csv, df_raw):\n",
    "    \"\"\"\n",
    "    Determine the next date to backdate based on the last date in the CSV.\n",
    "    If the CSV doesn't exist, return the earliest date in the raw data.\n",
    "    \"\"\"\n",
    "    df_raw['transaction_date'] = pd.to_datetime(df_raw['transaction_date'], errors='coerce')\n",
    "    \n",
    "    if not os.path.exists(output_csv):\n",
    "        return df_raw['transaction_date'].min()\n",
    "\n",
    "    df_existing = pd.read_csv(output_csv)\n",
    "    df_existing['transaction_date'] = pd.to_datetime(df_existing['transaction_date'], errors='coerce')\n",
    "    df_existing = df_existing.dropna(subset=['transaction_date'])\n",
    "    last_date = df_existing['transaction_date'].max()\n",
    "\n",
    "    return last_date + timedelta(days=1)\n",
    "\n",
    "\n",
    "def run_daily_incremental():\n",
    "    \"\"\"\n",
    "    Run ETL for the next unprocessed date in the raw CSV.\n",
    "    \"\"\"\n",
    "    df_raw = extract_data(INPUT_FILE)\n",
    "    if df_raw is None:\n",
    "        print(\"❌ Extraction failed.\")\n",
    "        return\n",
    "\n",
    "    next_date = get_next_date_to_process(OUTPUT_FILE, df_raw)\n",
    "\n",
    "    df_raw['transaction_date'] = pd.to_datetime(df_raw['transaction_date'], errors='coerce')\n",
    "    df_raw = df_raw.dropna(subset=['transaction_date'])\n",
    "\n",
    "    last_raw_date = df_raw['transaction_date'].max()\n",
    "    if next_date > last_raw_date:\n",
    "        print(\"✅ No new data to process. All dates up to latest available are loaded.\")\n",
    "        return\n",
    "\n",
    "    start_date_str = next_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = next_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"📆 Processing daily backdate for: {start_date_str}\")\n",
    "\n",
    "    # --- Transform and load ---\n",
    "    df_transformed = transform_data(df_raw, start_date_str, end_date_str)\n",
    "    if df_transformed.empty:\n",
    "        print(f\"⚠ No data found for {start_date_str}\")\n",
    "        return\n",
    "\n",
    "    load_data(df_transformed, OUTPUT_FILE, start_date_str, end_date_str)\n",
    "    print(f\"✅ Daily backdate for {start_date_str} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e7682a-6171-4fe9-9339-96a4dcacfd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 19:53:47,884 - INFO - 🔍 Checking for the latest date in MySQL...\n",
      "2025-08-20 19:53:47,946 - INFO - ✅ Latest date found in database: 2025-01-05\n",
      "2025-08-20 19:53:47,949 - INFO - 📆 Processing date: 2025-01-06\n",
      "2025-08-20 19:53:47,951 - INFO - 📥 Extracting data from 'Coffee Shop Sales2.csv'...\n",
      "2025-08-20 19:53:48,222 - INFO - 🔄 Transforming data...\n",
      "2025-08-20 19:53:51,288 - INFO - 🚀 Loading data to MySQL...\n",
      "2025-08-20 19:53:51,358 - INFO - Successfully connected to MySQL database.\n",
      "2025-08-20 19:53:51,510 - INFO - ✅ Load complete! Inserted/updated 509 records.\n",
      "2025-08-20 19:53:51,519 - INFO - MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import mysql.connector\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Database Connection Configurations\n",
    "# =========================================================\n",
    "# Configuration for the MySQL OLTP database.\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"Whatsnew2711\"\n",
    "MYSQL_DB = 'coffee_shop_sales'\n",
    "\n",
    "def get_mysql_connection():\n",
    "    \"\"\"Establishes and returns a MySQL database connection.\"\"\"\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            port=MYSQL_PORT,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        return conn\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ Failed to connect to MySQL: {err}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ ETL Functions\n",
    "# =========================================================\n",
    "\n",
    "def extract_data(input_csv):\n",
    "    \"\"\"\n",
    "    Extracts data from a specified CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(f\"📥 Extracting data from '{input_csv}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, sep=\";\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"❌ Error: The file '{input_csv}' was not found.\")\n",
    "        return None\n",
    "\n",
    "def transform_data(df, start_offset_date, end_offset_date):\n",
    "    \"\"\"\n",
    "    Transforms the raw DataFrame by:\n",
    "    - Filtering by date range\n",
    "    - Formatting dates & times\n",
    "    - Mapping store locations\n",
    "    - Converting currency\n",
    "    - Regenerating unique transaction IDs\n",
    "    \"\"\"\n",
    "    logging.info(\"🔄 Transforming data...\")\n",
    "\n",
    "    # Convert date and time\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "    df['transaction_time'] = pd.to_datetime(\n",
    "        df['transaction_time'], format='%H:%M:%S', errors='coerce'\n",
    "    ).dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Filter by date range\n",
    "    start_date = pd.to_datetime(start_offset_date)\n",
    "    end_date = pd.to_datetime(end_offset_date)\n",
    "    df_filtered = df[\n",
    "        (df['transaction_date'] >= start_date) & (df['transaction_date'] <= end_date)\n",
    "    ].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        logging.warning(f\"⚠ No data found for the range {start_offset_date} to {end_offset_date}.\")\n",
    "        return df_filtered\n",
    "\n",
    "    # Format date for output\n",
    "    df_filtered['transaction_date'] = df_filtered['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Map store locations\n",
    "    location_map = {\n",
    "        \"Hell's Kitchen\": \"East Campus\",\n",
    "        \"Astoria\": \"West Campus\",\n",
    "        \"Lower Manhattan\": \"Main Campus\"\n",
    "    }\n",
    "    df_filtered['store_location'] = df_filtered['store_location'].replace(location_map)\n",
    "\n",
    "    # Convert unit_price\n",
    "    df_filtered['unit_price'] = (df_filtered['unit_price'] * 15).round(2)\n",
    "\n",
    "    # Convert key columns to string safely\n",
    "    cols_to_convert = ['transaction_id', 'store_id', 'product_id']\n",
    "    df_filtered[cols_to_convert] = df_filtered[cols_to_convert].apply(\n",
    "        lambda col: col.dropna().astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # Force unique transaction IDs by appending the date\n",
    "    df_filtered['transaction_id'] = (\n",
    "        df_filtered['transaction_id'].astype(str) + \"_\" +\n",
    "        pd.to_datetime(df_filtered['transaction_date']).dt.strftime('%Y%m%d')\n",
    "    )\n",
    "    \n",
    "    # Reorder columns to match the MySQL table structure\n",
    "    df_transformed = df_filtered.reindex(columns=[\n",
    "        'transaction_id', 'transaction_date', 'transaction_time', 'transaction_qty',\n",
    "        'store_id', 'store_location', 'product_id', 'unit_price',\n",
    "        'product_category', 'product_type', 'product_detail'\n",
    "    ], fill_value=None)\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def load_data_to_mysql(df, table_name):\n",
    "    \"\"\"\n",
    "    Loads transformed data into a MySQL table using the ON DUPLICATE KEY UPDATE.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.warning(\"⚠ No data to load to MySQL.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"🚀 Loading data to MySQL...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = get_mysql_connection()\n",
    "        if not conn:\n",
    "            return\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        logging.info(\"Successfully connected to MySQL database.\")\n",
    "\n",
    "        # Prepare the list of tuples for insertion\n",
    "        records = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "        # The INSERT query with ON DUPLICATE KEY UPDATE to handle idempotency\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} (\n",
    "                transaction_id, transaction_date, transaction_time, transaction_qty,\n",
    "                store_id, store_location, product_id, unit_price,\n",
    "                product_category, product_type, product_detail\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                transaction_date=VALUES(transaction_date),\n",
    "                transaction_time=VALUES(transaction_time),\n",
    "                transaction_qty=VALUES(transaction_qty),\n",
    "                store_id=VALUES(store_id),\n",
    "                store_location=VALUES(store_location),\n",
    "                product_id=VALUES(product_id),\n",
    "                unit_price=VALUES(unit_price),\n",
    "                product_category=VALUES(product_category),\n",
    "                product_type=VALUES(product_type),\n",
    "                product_detail=VALUES(product_detail)\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.executemany(insert_query, records)\n",
    "        conn.commit()\n",
    "        logging.info(f\"✅ Load complete! Inserted/updated {cursor.rowcount} records.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        if conn and conn.is_connected():\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if cursor is not None:\n",
    "            cursor.close()\n",
    "        if conn and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"MySQL connection closed.\")\n",
    "\n",
    "def get_latest_date_from_mysql():\n",
    "    \"\"\"\n",
    "    Queries MySQL to find the most recent transaction date in the\n",
    "    sales_data table.\n",
    "    \"\"\"\n",
    "    logging.info(\"🔍 Checking for the latest date in MySQL...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = get_mysql_connection()\n",
    "        if not conn:\n",
    "            return None\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT MAX(transaction_date)\n",
    "            FROM sales_data;\n",
    "        \"\"\")\n",
    "        latest_date = cursor.fetchone()[0]\n",
    "        \n",
    "        if latest_date:\n",
    "            logging.info(f\"✅ Latest date found in database: {latest_date}\")\n",
    "            return latest_date\n",
    "        else:\n",
    "            logging.warning(\"⚠️ No data found in sales_data table. Starting from default date.\")\n",
    "            return None\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ Failed to retrieve latest date from MySQL: {err}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ Main Execution Logic for Daily Schedule\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"Coffee Shop Sales2.csv\"\n",
    "    table_name = \"sales_data\"\n",
    "    \n",
    "    # Get the latest date from the database\n",
    "    latest_date_in_db = get_latest_date_from_mysql()\n",
    "    \n",
    "    # Determine the date to process. If no data exists, start from the beginning.\n",
    "    if latest_date_in_db:\n",
    "        # Load the next day's data\n",
    "        date_to_process = (latest_date_in_db + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        # Default start date for the backfilling.\n",
    "        date_to_process = '2025-01-01'\n",
    "\n",
    "    logging.info(f\"📆 Processing date: {date_to_process}\")\n",
    "\n",
    "    df_raw = extract_data(input_file)\n",
    "    if df_raw is not None:\n",
    "        # The start and end date for transformation is the same, as we process one day at a time\n",
    "        df_transformed = transform_data(df_raw, date_to_process, date_to_process)\n",
    "        if df_transformed is not None:\n",
    "            load_data_to_mysql(df_transformed, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a01da-68ac-4b44-a4d0-a82306b33b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
