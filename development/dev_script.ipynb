{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd81015-5bcd-421c-9003-57ea0df0db99",
   "metadata": {},
   "source": [
    "First thing is we need a couple of scripts for our pipeline:\n",
    "Main script which will call all the other scripts and will provide us with just \"start_offset_date\" and \"end_offset_date\" for backdating along with the option to either comment out second run or third run. \n",
    "\n",
    "We will have an etl script and a main script that we will use to call the etl script defined as below\n",
    "1. We will have an extraction function that will get data from our csv and add that data to pandas dataframe\n",
    "2. Next we'll have a transformation function that will transform our location and price columns as stated above\n",
    "3. Then we'll have loading function that will load to our transformed csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8463a-27e0-47c3-a2a2-57dd3814ac3c",
   "metadata": {},
   "source": [
    "Testing the exract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76704429-dda9-4933-9571-f6e761d88ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_data(input_csv):\n",
    "    \"\"\"\n",
    "    Extracts data from a specified CSV file into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): The path to the raw sales data CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted data, or None if the file is not found.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting data from '{input_csv}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, sep = \";\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_csv}' was not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d7506f-bcc1-4092-9e3c-d132c3a368cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from 'Coffee Shop Sales2.csv'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:06:11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:08:56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:14:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:20:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:22:41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id transaction_date transaction_time  transaction_qty  \\\n",
       "0             1.0       2025/01/01         07:06:11              2.0   \n",
       "1             2.0       2025/01/01         07:08:56              2.0   \n",
       "2             3.0       2025/01/01         07:14:04              2.0   \n",
       "3             4.0       2025/01/01         07:20:24              1.0   \n",
       "4             5.0       2025/01/01         07:22:41              2.0   \n",
       "\n",
       "   store_id   store_location  product_id  unit_price    product_category  \\\n",
       "0       5.0  Lower Manhattan        32.0         3.0              Coffee   \n",
       "1       5.0  Lower Manhattan        57.0         3.1                 Tea   \n",
       "2       5.0  Lower Manhattan        59.0         4.5  Drinking Chocolate   \n",
       "3       5.0  Lower Manhattan        22.0         2.0              Coffee   \n",
       "4       5.0  Lower Manhattan        57.0         3.1                 Tea   \n",
       "\n",
       "            product_type               product_detail  \n",
       "0  Gourmet brewed coffee                  Ethiopia Rg  \n",
       "1        Brewed Chai tea     Spicy Eye Opener Chai Lg  \n",
       "2          Hot chocolate            Dark chocolate Lg  \n",
       "3            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "4        Brewed Chai tea     Spicy Eye Opener Chai Lg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_data(\"Coffee Shop Sales2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108873df-527a-4367-b4d3-2742dde3c4b4",
   "metadata": {},
   "source": [
    "Transformations\n",
    "\n",
    "Now that we have loaded the data we need to do a couple of transformations.\n",
    "\n",
    "1. Convert transaction_id, store_id and product_id columns to strings\n",
    "2. Properly format transaction_date and transaction_time to match their proper formats. Date and timestamp.\n",
    "3. Map locations to my local locations as follows, \"Hell's Kitchen\": \"East Campus\", \"Astoria\": \"West Campus\", \"Lower Manhattan\": \"Main Campus\"\n",
    "4. Convert price to rands and we'll assume a conversion rate of $1:R15 so that prices are more local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3863988-764d-476d-8e97-48adc6cf87ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Main Campus' 'East Campus' 'West Campus' nan]\n"
     ]
    }
   ],
   "source": [
    "#mapping locations\n",
    "\n",
    "location_map = {\n",
    "        \"Hell's Kitchen\": \"East Campus\",\n",
    "        \"Astoria\": \"West Campus\",\n",
    "        \"Lower Manhattan\": \"Main Campus\"\n",
    "    }\n",
    "df['store_location'] = df['store_location'].replace(location_map)\n",
    "unique_locations = df['store_location'].unique()\n",
    "print(unique_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c843dc1-48fc-41d5-91e8-586fcff8791f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:06:11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:08:56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>57</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:14:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>59</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:20:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:22:41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>57</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id transaction_date transaction_time  transaction_qty store_id  \\\n",
       "0              1       2025/01/01         07:06:11              2.0        5   \n",
       "1              2       2025/01/01         07:08:56              2.0        5   \n",
       "2              3       2025/01/01         07:14:04              2.0        5   \n",
       "3              4       2025/01/01         07:20:24              1.0        5   \n",
       "4              5       2025/01/01         07:22:41              2.0        5   \n",
       "\n",
       "  store_location product_id  unit_price    product_category  \\\n",
       "0    Main Campus         32         3.0              Coffee   \n",
       "1    Main Campus         57         3.1                 Tea   \n",
       "2    Main Campus         59         4.5  Drinking Chocolate   \n",
       "3    Main Campus         22         2.0              Coffee   \n",
       "4    Main Campus         57         3.1                 Tea   \n",
       "\n",
       "            product_type               product_detail  \n",
       "0  Gourmet brewed coffee                  Ethiopia Rg  \n",
       "1        Brewed Chai tea     Spicy Eye Opener Chai Lg  \n",
       "2          Hot chocolate            Dark chocolate Lg  \n",
       "3            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "4        Brewed Chai tea     Spicy Eye Opener Chai Lg  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting all id columns to strings that are properly formatted\n",
    "\n",
    "cols_to_convert = ['transaction_id', 'store_id', 'product_id']\n",
    "\n",
    "#If we might have missing values (NaN) in these columns and don’t want errors\n",
    "\n",
    "df[cols_to_convert] = (\n",
    "    df[cols_to_convert]\n",
    "    .apply(lambda col: col.dropna().astype(int).astype(str))\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c9d35-9c73-440e-9b0d-177b12e67bd1",
   "metadata": {},
   "source": [
    "Next we'll convert our price column to rands by multiplying it by 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b14ad138-bc39-4890-9660-63c3ce9c9cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:06:11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>32</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:08:56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>57</td>\n",
       "      <td>46.5</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:14:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>59</td>\n",
       "      <td>67.5</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:20:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>22</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025/01/01</td>\n",
       "      <td>07:22:41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>57</td>\n",
       "      <td>46.5</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id transaction_date transaction_time  transaction_qty store_id  \\\n",
       "0              1       2025/01/01         07:06:11              2.0        5   \n",
       "1              2       2025/01/01         07:08:56              2.0        5   \n",
       "2              3       2025/01/01         07:14:04              2.0        5   \n",
       "3              4       2025/01/01         07:20:24              1.0        5   \n",
       "4              5       2025/01/01         07:22:41              2.0        5   \n",
       "\n",
       "  store_location product_id  unit_price    product_category  \\\n",
       "0    Main Campus         32        45.0              Coffee   \n",
       "1    Main Campus         57        46.5                 Tea   \n",
       "2    Main Campus         59        67.5  Drinking Chocolate   \n",
       "3    Main Campus         22        30.0              Coffee   \n",
       "4    Main Campus         57        46.5                 Tea   \n",
       "\n",
       "            product_type               product_detail  \n",
       "0  Gourmet brewed coffee                  Ethiopia Rg  \n",
       "1        Brewed Chai tea     Spicy Eye Opener Chai Lg  \n",
       "2          Hot chocolate            Dark chocolate Lg  \n",
       "3            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "4        Brewed Chai tea     Spicy Eye Opener Chai Lg  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unit_price'] = (df['unit_price'] * 15).round(2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48144032-a838-4304-9019-1db33ff6d6f9",
   "metadata": {},
   "source": [
    "Let's format date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1950876b-5332-4732-83a1-033df6d3f941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>07:06:11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>32</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>07:08:56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>57</td>\n",
       "      <td>46.5</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>07:14:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>59</td>\n",
       "      <td>67.5</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>07:20:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>22</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>07:22:41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>57</td>\n",
       "      <td>46.5</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id transaction_date transaction_time  transaction_qty store_id  \\\n",
       "0              1       2025-01-01         07:06:11              2.0        5   \n",
       "1              2       2025-01-01         07:08:56              2.0        5   \n",
       "2              3       2025-01-01         07:14:04              2.0        5   \n",
       "3              4       2025-01-01         07:20:24              1.0        5   \n",
       "4              5       2025-01-01         07:22:41              2.0        5   \n",
       "\n",
       "  store_location product_id  unit_price    product_category  \\\n",
       "0    Main Campus         32        45.0              Coffee   \n",
       "1    Main Campus         57        46.5                 Tea   \n",
       "2    Main Campus         59        67.5  Drinking Chocolate   \n",
       "3    Main Campus         22        30.0              Coffee   \n",
       "4    Main Campus         57        46.5                 Tea   \n",
       "\n",
       "            product_type               product_detail  \n",
       "0  Gourmet brewed coffee                  Ethiopia Rg  \n",
       "1        Brewed Chai tea     Spicy Eye Opener Chai Lg  \n",
       "2          Hot chocolate            Dark chocolate Lg  \n",
       "3            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "4        Brewed Chai tea     Spicy Eye Opener Chai Lg  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format date as YYYY-MM-DD\n",
    "df['transaction_date'] = pd.to_datetime(\n",
    "    df['transaction_date'], errors='coerce'\n",
    ").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Format time as HH:MM:SS\n",
    "df['transaction_time'] = pd.to_datetime(\n",
    "    df['transaction_time'], format='%H:%M:%S', errors='coerce'\n",
    ").dt.strftime('%H:%M:%S')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daab441-7ea9-48a1-b2ed-c4de4ab8ab5f",
   "metadata": {},
   "source": [
    "Now all we need to do is put all the above into a function that will allow us to have dates we can use to backdate our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d03c96e2-6734-4ced-9d0b-d866f8f1bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, start_offset_date, end_offset_date):\n",
    "    \"\"\"\n",
    "    Transforms the raw DataFrame by filtering by date, mapping locations,\n",
    "    and converting currency.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw data DataFrame.\n",
    "        start_offset_date (str): The start date for filtering (e.g., '2025-01-01').\n",
    "        end_offset_date (str): The end date for filtering (e.g., '2025-03-31').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame, or an empty DataFrame if no data\n",
    "                      is found in the specified date range.\n",
    "    \"\"\"\n",
    "    print(\"Transforming data...\")\n",
    "\n",
    "\n",
    "    # Convert date and time to datetime/time\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "    df['transaction_time'] = pd.to_datetime(df['transaction_time'], format='%H:%M:%S', errors='coerce').dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Filter based on date range\n",
    "    start_date = pd.to_datetime(start_offset_date)\n",
    "    end_date = pd.to_datetime(end_offset_date)\n",
    "    df_filtered = df[(df['transaction_date'] >= start_date) & (df['transaction_date'] <= end_date)].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data found for the date range: {start_offset_date} to {end_offset_date}.\")\n",
    "        return df_filtered\n",
    "\n",
    "    # Format date for output\n",
    "    df_filtered['transaction_date'] = df_filtered['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Map store locations\n",
    "    location_map = {\n",
    "        \"Hell's Kitchen\": \"East Campus\",\n",
    "        \"Astoria\": \"West Campus\",\n",
    "        \"Lower Manhattan\": \"Main Campus\"\n",
    "    }\n",
    "    df_filtered['store_location'] = df_filtered['store_location'].replace(location_map)\n",
    "\n",
    "    # Convert unit_price\n",
    "    df_filtered['unit_price'] = (df_filtered['unit_price'] * 15).round(2)\n",
    "\n",
    "\n",
    "    cols_to_convert = ['transaction_id', 'store_id', 'product_id']\n",
    "\n",
    "    #If we might have missing values (NaN) in these columns and don’t want errors\n",
    "\n",
    "    df_filtered[cols_to_convert] = (\n",
    "        df_filtered[cols_to_convert]\n",
    "        .apply(lambda col: col.dropna().astype(int).astype(str)))\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6769b2a9-dc93-4967-a1fc-f8a989782583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33673</th>\n",
       "      <td>33730</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>07:01:20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>22</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33674</th>\n",
       "      <td>33731</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>07:02:34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>30</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Columbian Medium Roast Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33675</th>\n",
       "      <td>33732</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>07:02:56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>22</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33676</th>\n",
       "      <td>33733</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>07:04:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>37</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Barista Espresso</td>\n",
       "      <td>Espresso shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677</th>\n",
       "      <td>33734</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>07:06:11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>32</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      transaction_id transaction_date transaction_time  transaction_qty  \\\n",
       "33673          33730       2025-03-01         07:01:20              2.0   \n",
       "33674          33731       2025-03-01         07:02:34              2.0   \n",
       "33675          33732       2025-03-01         07:02:56              1.0   \n",
       "33676          33733       2025-03-01         07:04:24              1.0   \n",
       "33677          33734       2025-03-01         07:06:11              2.0   \n",
       "\n",
       "      store_id store_location product_id  unit_price product_category  \\\n",
       "33673        5    Main Campus         22        30.0           Coffee   \n",
       "33674        5    Main Campus         30        45.0           Coffee   \n",
       "33675        5    Main Campus         22        30.0           Coffee   \n",
       "33676        5    Main Campus         37        45.0           Coffee   \n",
       "33677        5    Main Campus         32        45.0           Coffee   \n",
       "\n",
       "                product_type               product_detail  \n",
       "33673            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "33674  Gourmet brewed coffee    Columbian Medium Roast Lg  \n",
       "33675            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "33676       Barista Espresso                Espresso shot  \n",
       "33677  Gourmet brewed coffee                  Ethiopia Rg  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = transform_data(df, \"2025-03-01\", \"2025-03-05\")\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c222805-cc81-49a0-a7c8-736a95f39dbd",
   "metadata": {},
   "source": [
    "Now we will work on loading our data. First, we will load the data into a csv on our local pc. Once we are able to load our data into a csv, we need to create logic that will make sure that we do not duplicate transactions and thus if we load transactions that have been loaded already, they will get replaced otherwise the transactions will be appended. Once our logic is successful, we will attempt automating data into our csv so we upload data daily automated. The last step in building our first ETL is to get our data to flow into our OLTP (MySQL) and Bigquery tables but later on that :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2daa2c5-fa96-4598-8915-1bf7459cdee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df_transformed, output_csv, start_offset_date, end_offset_date):\n",
    "    if df_transformed.empty:\n",
    "        print(\"⚠ No data to load.\")\n",
    "        return\n",
    "\n",
    "    # Ensure correct types\n",
    "    df_transformed['transaction_date'] = pd.to_datetime(df_transformed['transaction_date'])\n",
    "    df_transformed['transaction_id'] = df_transformed['transaction_id'].astype(str)\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        print(\"📂 Existing data found. Removing old data in the same date range...\")\n",
    "        df_existing = pd.read_csv(output_csv)\n",
    "        df_existing['transaction_date'] = pd.to_datetime(df_existing['transaction_date'])\n",
    "        df_existing['transaction_id'] = df_existing['transaction_id'].astype(str)\n",
    "\n",
    "        # Remove rows from existing file that fall within the same date range\n",
    "        mask = ~(\n",
    "            (df_existing['transaction_date'] >= pd.to_datetime(start_offset_date)) &\n",
    "            (df_existing['transaction_date'] <= pd.to_datetime(end_offset_date))\n",
    "        )\n",
    "        df_existing = df_existing[mask]\n",
    "\n",
    "        # Append new transformed data\n",
    "        df_final = pd.concat([df_existing, df_transformed], ignore_index=True)\n",
    "    else:\n",
    "        print(\"🆕 No existing data. Creating a new file...\")\n",
    "        df_final = df_transformed.copy()\n",
    "\n",
    "    # Keep only unique transactions\n",
    "    df_final = df_final.drop_duplicates(subset=['transaction_id'], keep='last')\n",
    "    df_final = df_final.sort_values(by=['transaction_date', 'transaction_time'])\n",
    "\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Load complete! Total unique transactions: {df_final['transaction_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b018a15a-3a11-4c5b-9d67-09bb9e54fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Existing data found. Removing old data in the same date range...\n",
      "✅ Load complete! Total unique transactions: 3343\n"
     ]
    }
   ],
   "source": [
    "load_data(df_new, \"successful_upload.csv\", \"2025-01-01\", \"2025-04-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a4138-01fc-45cf-9500-c07fa23731fa",
   "metadata": {},
   "source": [
    "I used cd dags\n",
    "rm -rf .ipynb_checkpoints\n",
    "\n",
    "to make the dags folder accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c3cfd9-8664-49a7-af59-a4ca1fc941c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Using cached mysql_connector_python-9.4.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Using cached mysql_connector_python-9.4.0-py2.py3-none-any.whl (406 kB)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5fa80-b160-4e52-aff0-e85cad50183d",
   "metadata": {},
   "source": [
    "'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': '@Whatsnew2711',\n",
    "        'database': 'coffee_shop_sales_oltp' for MySQL\n",
    "\n",
    "PostgreSQL password: 'Whatsnew2711'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0101a307-f819-486d-9aeb-5254a31c2362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Using cached PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Using cached PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acaee071-999f-4ee9-8a1b-2db0b91adedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: SQLAlchemy\n",
      "Version: 1.4.54\n",
      "Summary: Database Abstraction Library\n",
      "Home-page: https://www.sqlalchemy.org\n",
      "Author: Mike Bayer\n",
      "Author-email: mike_mp@zzzcomputing.com\n",
      "License: MIT\n",
      "Location: /Users/mac/anaconda3/lib/python3.10/site-packages\n",
      "Requires: greenlet\n",
      "Required-by: aext-project-filebrowser-server, alembic, apache-airflow-core, ipython-sql, SQLAlchemy-JSONField, SQLAlchemy-Utils\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596bffcf-26ed-45ed-b8d4-9646d3fbd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy in /Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages (1.4.49)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages (from SQLAlchemy) (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5eda5f2-8f67-4ad5-8a26-b8294786f97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.10.tar.gz (385 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "\u001b[33m  DEPRECATION: Building 'psycopg2' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'psycopg2'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.10-cp311-cp311-macosx_10_15_x86_64.whl size=134232 sha256=727426fb4ff5c10e4c69c805c85c4fbeece2cbacee648f3796112d9be1c7a31b\n",
      "  Stored in directory: /Users/mac/Library/Caches/pip/wheels/d9/83/60/e9660320860aef3c38a67dea6ff9538e4cad76502cb39ed280\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f259df0-e2a7-45fb-9ca7-e8dcba934517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Set up logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Database Connection Configurations\n",
    "# =========================================================\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"@Whatsnew2711\"\n",
    "MYSQL_DB = \"coffee_shop_sales_oltp\"\n",
    "\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"Whatsnew2711\"\n",
    "PG_DB = \"Coffee_Machine_Project_One\"\n",
    "\n",
    "def get_postgres_connection():\n",
    "    \"\"\"Establishes and returns a PostgreSQL database connection.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD,\n",
    "            dbname=PG_DB\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ ETL Functions\n",
    "# =========================================================\n",
    "\n",
    "def extract_data_from_mysql(date_to_process):\n",
    "    \"\"\"\n",
    "    Extracts daily transaction data from the MySQL OLTP database\n",
    "    for a specific date using a direct connection.\n",
    "    \"\"\"\n",
    "    logging.info(f\"📥 Extracting data from MySQL for {date_to_process}...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        # Pass parameters directly as keyword arguments to avoid parsing issues.\n",
    "        conn = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            port=MYSQL_PORT,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            transaction_date,\n",
    "            transaction_time,\n",
    "            transaction_qty,\n",
    "            store_id,\n",
    "            store_location,\n",
    "            product_id,\n",
    "            unit_price,\n",
    "            product_category,\n",
    "            product_type,\n",
    "            product_detail\n",
    "        FROM sales_data\n",
    "        WHERE transaction_date = '{date_to_process}';\n",
    "        \"\"\"\n",
    "        df_raw = pd.read_sql(query, conn)\n",
    "        logging.info(f\"✅ Extracted {len(df_raw)} records.\")\n",
    "        return df_raw\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        # Rollback not needed for a read operation, but included for consistency\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"MySQL connection closed.\")\n",
    "\n",
    "def transform_data(df_raw):\n",
    "    \"\"\"\n",
    "    Transforms the raw data into a star schema format.\n",
    "    Returns the fact table and all dimension tables.\n",
    "    \"\"\"\n",
    "    logging.info(\"🔄 Transforming data for OLAP...\")\n",
    "    df_raw[\"total_amount\"] = df_raw[\"transaction_qty\"] * df_raw[\"unit_price\"]\n",
    "\n",
    "    # Dim Date\n",
    "    dim_date = df_raw[[\"transaction_date\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_date[\"date_key\"] = dim_date[\"transaction_date\"].dt.strftime('%Y%m%d').astype(int)\n",
    "    dim_date[\"full_date\"] = dim_date[\"transaction_date\"]\n",
    "    dim_date[\"day_of_week\"] = dim_date[\"transaction_date\"].dt.day_name()\n",
    "    dim_date[\"month_name\"] = dim_date[\"transaction_date\"].dt.month_name()\n",
    "    dim_date[\"quarter\"] = dim_date[\"transaction_date\"].dt.quarter\n",
    "    dim_date[\"year\"] = dim_date[\"transaction_date\"].dt.year\n",
    "    dim_date = dim_date[[\"date_key\", \"full_date\", \"day_of_week\", \"month_name\", \"quarter\", \"year\"]]\n",
    "\n",
    "    # Dim Time\n",
    "    dim_time = df_raw[[\"transaction_time\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_time[\"time_key\"] = dim_time[\"transaction_time\"].dt.hour * 10000 + \\\n",
    "                           dim_time[\"transaction_time\"].dt.minute * 100 + \\\n",
    "                           dim_time[\"transaction_time\"].dt.second\n",
    "    dim_time[\"full_time\"] = dim_time[\"transaction_time\"]\n",
    "    dim_time[\"hour\"] = dim_time[\"transaction_time\"].dt.hour\n",
    "    dim_time[\"minute\"] = dim_time[\"transaction_time\"].dt.minute\n",
    "    dim_time = dim_time[[\"time_key\", \"full_time\", \"hour\", \"minute\"]]\n",
    "\n",
    "    # Dim Store\n",
    "    dim_store = df_raw[[\"store_id\", \"store_location\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_store[\"store_key\"] = dim_store.index + 1\n",
    "\n",
    "    # Dim Product\n",
    "    dim_product = df_raw[[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_product[\"product_key\"] = dim_product.index + 1\n",
    "\n",
    "    # Fact Table\n",
    "    df_fact = df_raw.copy()\n",
    "    df_fact = df_fact.merge(dim_date[[\"transaction_date\",\"date_key\"]], on=\"transaction_date\")\n",
    "    df_fact = df_fact.merge(dim_time[[\"transaction_time\",\"time_key\"]], on=\"transaction_time\")\n",
    "    df_fact = df_fact.merge(dim_store[[\"store_id\",\"store_key\"]], on=\"store_id\")\n",
    "    df_fact = df_fact.merge(dim_product[[\"product_id\",\"product_key\"]], on=\"product_id\")\n",
    "\n",
    "    df_fact = df_fact[[\n",
    "        \"transaction_id\",\n",
    "        \"date_key\",\n",
    "        \"time_key\",\n",
    "        \"store_key\",\n",
    "        \"product_key\",\n",
    "        \"transaction_qty\",\n",
    "        \"total_amount\"\n",
    "    ]]\n",
    "\n",
    "    return dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "\n",
    "def load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact):\n",
    "    \"\"\"\n",
    "    Loads transformed data into the PostgreSQL OLAP database.\n",
    "    \"\"\"\n",
    "    logging.info(\"📤 Loading data to PostgreSQL...\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_postgres_connection()\n",
    "        if not conn:\n",
    "            return\n",
    "            \n",
    "        def insert_if_not_exists(df, table_name, unique_col):\n",
    "            if df.empty:\n",
    "                return\n",
    "            cursor = conn.cursor()\n",
    "            for _, row in df.iterrows():\n",
    "                cols = list(row.index)\n",
    "                vals = [row[c] for c in cols]\n",
    "                placeholders = ','.join(['%s'] * len(cols))\n",
    "                sql = f\"\"\"\n",
    "                INSERT INTO {table_name} ({','.join(cols)})\n",
    "                VALUES ({placeholders})\n",
    "                ON CONFLICT ({unique_col}) DO NOTHING;\n",
    "                \"\"\"\n",
    "                cursor.execute(sql, vals)\n",
    "            conn.commit()\n",
    "            cursor.close()\n",
    "        \n",
    "        insert_if_not_exists(dim_date, 'dim_date', 'date_key')\n",
    "        logging.info(\"✅ dim_date loaded.\")\n",
    "        insert_if_not_exists(dim_time, 'dim_time', 'time_key')\n",
    "        logging.info(\"✅ dim_time loaded.\")\n",
    "        insert_if_not_exists(dim_store, 'dim_store', 'store_key')\n",
    "        logging.info(\"✅ dim_store loaded.\")\n",
    "        insert_if_not_exists(dim_product, 'dim_product', 'product_key')\n",
    "        logging.info(\"✅ dim_product loaded.\")\n",
    "        insert_if_not_exists(df_fact, 'fact_sales', 'transaction_id')\n",
    "        logging.info(\"✅ fact_sales loaded.\")\n",
    "        logging.info(\"✅ Data loaded to PostgreSQL successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to load data to PostgreSQL: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9ce8ab-11ae-486e-b023-d414506e7a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 06:17:15,680 - INFO - 📥 Extracting data from MySQL for 2025-01-01...\n",
      "/var/folders/jh/nl0hs4bx26l4cbrp239k3n9c0000gn/T/ipykernel_1405/2732526060.py:80: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_raw = pd.read_sql(query, conn)\n",
      "2025-08-20 06:17:16,000 - INFO - ✅ Extracted 549 records.\n",
      "2025-08-20 06:17:16,002 - INFO - MySQL connection closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 07:06:11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>32</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 07:39:34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>58</td>\n",
       "      <td>52.50</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:49:51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>61</td>\n",
       "      <td>71.25</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Sustainably Grown Organic Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:51:17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>47</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Green tea</td>\n",
       "      <td>Serenity Green Tea Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:52:44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>East Campus</td>\n",
       "      <td>23</td>\n",
       "      <td>37.50</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>95_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:30:07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>70</td>\n",
       "      <td>48.75</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Scone</td>\n",
       "      <td>Cranberry Scone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>96_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:35:47</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>East Campus</td>\n",
       "      <td>60</td>\n",
       "      <td>56.25</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Sustainably Grown Organic Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>97_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:41:53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>59</td>\n",
       "      <td>67.50</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>98_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:47:53</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Main Campus</td>\n",
       "      <td>36</td>\n",
       "      <td>56.25</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Premium brewed coffee</td>\n",
       "      <td>Jamaican Coffee River Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>99_20250101</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0 days 10:49:13</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>East Campus</td>\n",
       "      <td>51</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Black tea</td>\n",
       "      <td>Earl Grey Lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    transaction_id transaction_date transaction_time  transaction_qty  \\\n",
       "0       1_20250101       2025-01-01  0 days 07:06:11              2.0   \n",
       "1      10_20250101       2025-01-01  0 days 07:39:34              2.0   \n",
       "2     100_20250101       2025-01-01  0 days 10:49:51              2.0   \n",
       "3     101_20250101       2025-01-01  0 days 10:51:17              1.0   \n",
       "4     102_20250101       2025-01-01  0 days 10:52:44              2.0   \n",
       "..             ...              ...              ...              ...   \n",
       "544    95_20250101       2025-01-01  0 days 10:30:07              1.0   \n",
       "545    96_20250101       2025-01-01  0 days 10:35:47              1.0   \n",
       "546    97_20250101       2025-01-01  0 days 10:41:53              1.0   \n",
       "547    98_20250101       2025-01-01  0 days 10:47:53              2.0   \n",
       "548    99_20250101       2025-01-01  0 days 10:49:13              2.0   \n",
       "\n",
       "    store_id store_location product_id  unit_price    product_category  \\\n",
       "0          5    Main Campus         32       45.00              Coffee   \n",
       "1          5    Main Campus         58       52.50  Drinking Chocolate   \n",
       "2          5    Main Campus         61       71.25  Drinking Chocolate   \n",
       "3          5    Main Campus         47       45.00                 Tea   \n",
       "4          8    East Campus         23       37.50              Coffee   \n",
       "..       ...            ...        ...         ...                 ...   \n",
       "544        5    Main Campus         70       48.75              Bakery   \n",
       "545        8    East Campus         60       56.25  Drinking Chocolate   \n",
       "546        5    Main Campus         59       67.50  Drinking Chocolate   \n",
       "547        5    Main Campus         36       56.25              Coffee   \n",
       "548        8    East Campus         51       45.00                 Tea   \n",
       "\n",
       "              product_type                product_detail  \n",
       "0    Gourmet brewed coffee                   Ethiopia Rg  \n",
       "1            Hot chocolate             Dark chocolate Rg  \n",
       "2            Hot chocolate  Sustainably Grown Organic Lg  \n",
       "3         Brewed Green tea         Serenity Green Tea Lg  \n",
       "4              Drip coffee   Our Old Time Diner Blend Rg  \n",
       "..                     ...                           ...  \n",
       "544                  Scone               Cranberry Scone  \n",
       "545          Hot chocolate  Sustainably Grown Organic Rg  \n",
       "546          Hot chocolate             Dark chocolate Lg  \n",
       "547  Premium brewed coffee      Jamaican Coffee River Lg  \n",
       "548       Brewed Black tea                  Earl Grey Lg  \n",
       "\n",
       "[549 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_data_from_mysql(\"2025-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa32d2b-863e-48da-8faa-211fe7f31751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 06:18:23,293 - INFO - 🔄 Transforming data for OLAP...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m transform_data(df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtransform_data\u001b[39m\u001b[34m(df_raw)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Dim Date\u001b[39;00m\n\u001b[32m    105\u001b[39m dim_date = df_raw[[\u001b[33m\"\u001b[39m\u001b[33mtransaction_date\u001b[39m\u001b[33m\"\u001b[39m]].drop_duplicates().reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m dim_date[\u001b[33m\"\u001b[39m\u001b[33mdate_key\u001b[39m\u001b[33m\"\u001b[39m] = dim_date[\u001b[33m\"\u001b[39m\u001b[33mtransaction_date\u001b[39m\u001b[33m\"\u001b[39m].dt.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    107\u001b[39m dim_date[\u001b[33m\"\u001b[39m\u001b[33mfull_date\u001b[39m\u001b[33m\"\u001b[39m] = dim_date[\u001b[33m\"\u001b[39m\u001b[33mtransaction_date\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    108\u001b[39m dim_date[\u001b[33m\"\u001b[39m\u001b[33mday_of_week\u001b[39m\u001b[33m\"\u001b[39m] = dim_date[\u001b[33m\"\u001b[39m\u001b[33mtransaction_date\u001b[39m\u001b[33m\"\u001b[39m].dt.day_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/airflow_env/lib/python3.11/site-packages/pandas/core/generic.py:6318\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6312\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6313\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6314\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6315\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m ):\n\u001b[32m   6317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/airflow_env/lib/python3.11/site-packages/pandas/core/accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28mself\u001b[39m._accessor(obj)\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/airflow_env/lib/python3.11/site-packages/pandas/core/indexes/accessors.py:643\u001b[39m, in \u001b[36mCombinedDatetimelikeProperties.__new__\u001b[39m\u001b[34m(cls, data)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data.dtype, PeriodDtype):\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc745861-829a-4993-80ad-a284c0cd02a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3526988528.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pkill -f \"airflow\"\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pkill -f \"airflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089a741-6861-4219-b1c5-e71c61eb7b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63e44253-4f86-4756-8dcc-80ea11abcb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "import logging\n",
    "\n",
    "# Set up logging for visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Database Connection Configurations\n",
    "# =========================================================\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"@Whatsnew2711\"\n",
    "MYSQL_DB = \"coffee_shop_sales_oltp\"\n",
    "\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"Whatsnew2711\"\n",
    "PG_DB = \"Coffee_Machine_Project_One\"\n",
    "\n",
    "def get_postgres_connection():\n",
    "    \"\"\"Establishes and returns a PostgreSQL database connection.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD,\n",
    "            dbname=PG_DB\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ ETL Functions\n",
    "# =========================================================\n",
    "\n",
    "def extract_data_from_mysql(date_to_process):\n",
    "    \"\"\"\n",
    "    Extracts daily transaction data from MySQL OLTP database\n",
    "    for a specific date.\n",
    "    \"\"\"\n",
    "    logging.info(f\"📥 Extracting data from MySQL for {date_to_process}...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            port=MYSQL_PORT,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            transaction_date,\n",
    "            transaction_time,\n",
    "            transaction_qty,\n",
    "            store_id,\n",
    "            store_location,\n",
    "            product_id,\n",
    "            unit_price,\n",
    "            product_category,\n",
    "            product_type,\n",
    "            product_detail\n",
    "        FROM sales_data\n",
    "        WHERE transaction_date = '{date_to_process}';\n",
    "        \"\"\"\n",
    "        df_raw = pd.read_sql(query, conn)\n",
    "        logging.info(f\"✅ Extracted {len(df_raw)} records.\")\n",
    "        return df_raw\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"MySQL connection closed.\")\n",
    "\n",
    "# =============================\n",
    "# Transform Data (Postgres-ready)\n",
    "# =============================\n",
    "\n",
    "def transform_data(df_raw):\n",
    "    \"\"\"\n",
    "    Transforms raw MySQL data into star schema DataFrames that match PostgreSQL tables.\n",
    "    Returns: dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    logging.info(\"🔄 Transforming data for OLAP...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_date to datetime\n",
    "    # ----------------------------\n",
    "    if \"transaction_date\" in df_raw.columns:\n",
    "        df_raw[\"transaction_date\"] = pd.to_datetime(df_raw[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_time from timedelta to time\n",
    "    # ----------------------------\n",
    "    if \"transaction_time\" in df_raw.columns:\n",
    "        if pd.api.types.is_timedelta64_dtype(df_raw[\"transaction_time\"]):\n",
    "            df_raw[\"transaction_time\"] = df_raw[\"transaction_time\"].apply(\n",
    "                lambda x: (pd.Timestamp(\"00:00:00\") + x).time() if pd.notnull(x) else None\n",
    "            )\n",
    "        else:\n",
    "            df_raw[\"transaction_time\"] = pd.to_datetime(\n",
    "                df_raw[\"transaction_time\"].astype(str), errors=\"coerce\"\n",
    "            ).dt.time\n",
    "\n",
    "    # Drop rows where conversion failed\n",
    "    df_raw = df_raw.dropna(subset=[\"transaction_date\", \"transaction_time\"])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Calculate total_amount\n",
    "    # ----------------------------\n",
    "    df_raw[\"total_amount\"] = df_raw[\"transaction_qty\"] * df_raw[\"unit_price\"]\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Date\n",
    "    # ----------------------------\n",
    "    dim_date = df_raw[[\"transaction_date\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_date[\"day\"] = dim_date[\"transaction_date\"].dt.day\n",
    "    dim_date[\"month\"] = dim_date[\"transaction_date\"].dt.month\n",
    "    dim_date[\"year\"] = dim_date[\"transaction_date\"].dt.year\n",
    "    dim_date[\"weekday\"] = dim_date[\"transaction_date\"].dt.day_name()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Time\n",
    "    # ----------------------------\n",
    "    dim_time = df_raw[[\"transaction_time\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_time[\"hour\"] = dim_time[\"transaction_time\"].apply(lambda x: x.hour)\n",
    "    dim_time[\"minute\"] = dim_time[\"transaction_time\"].apply(lambda x: x.minute)\n",
    "    dim_time[\"second\"] = dim_time[\"transaction_time\"].apply(lambda x: x.second)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Store\n",
    "    # ----------------------------\n",
    "    dim_store = df_raw[[\"store_id\", \"store_location\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Product\n",
    "    # ----------------------------\n",
    "    dim_product = df_raw[[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Fact Table\n",
    "    # ----------------------------\n",
    "    df_fact = df_raw.copy()\n",
    "    df_fact = df_fact.merge(dim_date, on=\"transaction_date\")\n",
    "    df_fact = df_fact.merge(dim_time, on=\"transaction_time\")\n",
    "    df_fact = df_fact.merge(dim_store, on=[\"store_id\", \"store_location\"])\n",
    "    df_fact = df_fact.merge(dim_product, on=[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"])\n",
    "\n",
    "    df_fact = df_fact[[\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",  # maps to dim_date.date_id in load\n",
    "        \"transaction_time\",  # maps to dim_time.time_id\n",
    "        \"store_id\",          # maps to dim_store.store_id\n",
    "        \"product_id\",        # maps to dim_product.product_id\n",
    "        \"transaction_qty\",\n",
    "        \"total_amount\"\n",
    "    ]]\n",
    "\n",
    "    logging.info(f\"✅ Transformation completed. Records: {len(df_fact)}\")\n",
    "    return dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "\n",
    "\n",
    "def load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact):\n",
    "    \"\"\"\n",
    "    Loads transformed data into PostgreSQL OLAP database\n",
    "    while ensuring no duplicates.\n",
    "    \"\"\"\n",
    "    logging.info(\"📤 Loading data to PostgreSQL...\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_postgres_connection()\n",
    "        if not conn:\n",
    "            return\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Date\n",
    "        # -----------------------------\n",
    "        date_id_map = {}\n",
    "        for _, row in dim_date.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO dim_date (transaction_date, day, month, year, weekday)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (transaction_date) DO UPDATE SET\n",
    "                    day = EXCLUDED.day,\n",
    "                    month = EXCLUDED.month,\n",
    "                    year = EXCLUDED.year,\n",
    "                    weekday = EXCLUDED.weekday\n",
    "                RETURNING date_id;\n",
    "            \"\"\", (\n",
    "                row[\"transaction_date\"].date(),\n",
    "                int(row[\"day\"]),\n",
    "                int(row[\"month\"]),\n",
    "                int(row[\"year\"]),\n",
    "                row[\"weekday\"]\n",
    "            ))\n",
    "            date_id_map[row[\"transaction_date\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Time\n",
    "        # -----------------------------\n",
    "        time_id_map = {}\n",
    "        for _, row in dim_time.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO dim_time (transaction_time, hour, minute, second)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (transaction_time) DO UPDATE SET\n",
    "                    hour = EXCLUDED.hour,\n",
    "                    minute = EXCLUDED.minute,\n",
    "                    second = EXCLUDED.second\n",
    "                RETURNING time_id;\n",
    "            \"\"\", (\n",
    "                row[\"transaction_time\"],\n",
    "                int(row[\"hour\"]),\n",
    "                int(row[\"minute\"]),\n",
    "                int(row[\"second\"])\n",
    "            ))\n",
    "            time_id_map[row[\"transaction_time\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Store\n",
    "        # -----------------------------\n",
    "        store_id_map = {}\n",
    "        for _, row in dim_store.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO dim_store (store_location)\n",
    "                VALUES (%s)\n",
    "                ON CONFLICT (store_id) DO NOTHING\n",
    "                RETURNING store_id;\n",
    "            \"\"\", (row[\"store_location\"],))\n",
    "            result = cur.fetchone()\n",
    "            store_id_map[row[\"store_id\"]] = result[0] if result else row[\"store_id\"]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Product\n",
    "        # -----------------------------\n",
    "        product_id_map = {}\n",
    "        for _, row in dim_product.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO dim_product (product_category, product_type, product_detail, unit_price)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (product_id) DO NOTHING\n",
    "                RETURNING product_id;\n",
    "            \"\"\", (\n",
    "                row[\"product_category\"],\n",
    "                row[\"product_type\"],\n",
    "                row[\"product_detail\"],\n",
    "                float(row[\"unit_price\"])\n",
    "            ))\n",
    "            result = cur.fetchone()\n",
    "            product_id_map[row[\"product_id\"]] = result[0] if result else row[\"product_id\"]\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fact Table\n",
    "        # -----------------------------\n",
    "        for _, row in df_fact.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO fact_sales (date_id, time_id, store_id, product_id, transaction_qty, total_amount)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (transaction_id) DO NOTHING;\n",
    "            \"\"\", (\n",
    "                date_id_map[row[\"transaction_date\"]],\n",
    "                time_id_map[row[\"transaction_time\"]],\n",
    "                store_id_map[row[\"store_id\"]],\n",
    "                product_id_map[row[\"product_id\"]],\n",
    "                int(row[\"transaction_qty\"]),\n",
    "                float(row[\"total_amount\"])\n",
    "            ))\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        logging.info(\"✅ Data loaded to PostgreSQL successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to load data to PostgreSQL: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e086121e-71dc-4491-aac3-d64ae6a77a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 08:31:24,458 - INFO - 📥 Extracting data from MySQL for 2025-01-02...\n",
      "/var/folders/jh/nl0hs4bx26l4cbrp239k3n9c0000gn/T/ipykernel_1405/134622241.py:76: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_raw = pd.read_sql(query, conn)\n",
      "2025-08-20 08:31:24,772 - INFO - ✅ Extracted 566 records.\n",
      "2025-08-20 08:31:24,776 - INFO - MySQL connection closed.\n",
      "2025-08-20 08:31:24,791 - INFO - 🔄 Transforming data for OLAP...\n",
      "2025-08-20 08:31:24,879 - INFO - ✅ Transformation completed. Records: 566\n",
      "2025-08-20 08:31:24,888 - INFO - 📤 Loading data to PostgreSQL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data sample:\n",
      "  transaction_id transaction_date transaction_time  transaction_qty store_id  \\\n",
      "0  1000_20250102       2025-01-02  0 days 17:19:03              1.0        8   \n",
      "1  1001_20250102       2025-01-02  0 days 17:19:40              1.0        5   \n",
      "2  1002_20250102       2025-01-02  0 days 17:19:40              1.0        5   \n",
      "3  1003_20250102       2025-01-02  0 days 17:20:14              1.0        8   \n",
      "4  1004_20250102       2025-01-02  0 days 17:21:01              1.0        3   \n",
      "\n",
      "  store_location product_id  unit_price product_category  \\\n",
      "0    East Campus         54        37.5              Tea   \n",
      "1    Main Campus         27        52.5           Coffee   \n",
      "2    Main Campus         74        52.5           Bakery   \n",
      "3    East Campus         46        37.5              Tea   \n",
      "4    West Campus         28        30.0           Coffee   \n",
      "\n",
      "            product_type             product_detail  \n",
      "0        Brewed Chai tea    Morning Sunrise Chai Rg  \n",
      "1  Organic brewed coffee               Brazilian Lg  \n",
      "2               Biscotti            Ginger Biscotti  \n",
      "3       Brewed Green tea      Serenity Green Tea Rg  \n",
      "4  Gourmet brewed coffee  Columbian Medium Roast Sm  \n",
      "\n",
      "Dimension & Fact tables samples:\n",
      "dim_date:\n",
      "  transaction_date  day  month  year   weekday\n",
      "0       2025-01-02    2      1  2025  Thursday\n",
      "dim_time:\n",
      "  transaction_time  hour  minute  second\n",
      "0         17:19:03    17      19       3\n",
      "1         17:19:40    17      19      40\n",
      "2         17:20:14    17      20      14\n",
      "3         17:21:01    17      21       1\n",
      "4         17:21:19    17      21      19\n",
      "dim_store:\n",
      "  store_id store_location\n",
      "0        8    East Campus\n",
      "1        5    Main Campus\n",
      "2        3    West Campus\n",
      "dim_product:\n",
      "  product_id product_category           product_type  \\\n",
      "0         54              Tea        Brewed Chai tea   \n",
      "1         27           Coffee  Organic brewed coffee   \n",
      "2         74           Bakery               Biscotti   \n",
      "3         46              Tea       Brewed Green tea   \n",
      "4         28           Coffee  Gourmet brewed coffee   \n",
      "\n",
      "              product_detail  unit_price  \n",
      "0    Morning Sunrise Chai Rg        37.5  \n",
      "1               Brazilian Lg        52.5  \n",
      "2            Ginger Biscotti        52.5  \n",
      "3      Serenity Green Tea Rg        37.5  \n",
      "4  Columbian Medium Roast Sm        30.0  \n",
      "fact_sales:\n",
      "  transaction_id transaction_date transaction_time store_id product_id  \\\n",
      "0  1000_20250102       2025-01-02         17:19:03        8         54   \n",
      "1  1001_20250102       2025-01-02         17:19:40        5         27   \n",
      "2  1002_20250102       2025-01-02         17:19:40        5         74   \n",
      "3  1003_20250102       2025-01-02         17:20:14        8         46   \n",
      "4  1004_20250102       2025-01-02         17:21:01        3         28   \n",
      "\n",
      "   transaction_qty  total_amount  \n",
      "0              1.0          37.5  \n",
      "1              1.0          52.5  \n",
      "2              1.0          52.5  \n",
      "3              1.0          37.5  \n",
      "4              1.0          30.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 08:31:25,290 - INFO - ✅ Data loaded to PostgreSQL successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ ETL process completed for 2025-01-02\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Extract data for a specific date\n",
    "df_raw = extract_data_from_mysql(\"2025-01-02\")\n",
    "print(\"Extracted data sample:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# 2️⃣ Transform the raw data into star schema\n",
    "dim_date, dim_time, dim_store, dim_product, df_fact = transform_data(df_raw)\n",
    "print(\"\\nDimension & Fact tables samples:\")\n",
    "print(\"dim_date:\")\n",
    "print(dim_date.head())\n",
    "print(\"dim_time:\")\n",
    "print(dim_time.head())\n",
    "print(\"dim_store:\")\n",
    "print(dim_store.head())\n",
    "print(\"dim_product:\")\n",
    "print(dim_product.head())\n",
    "print(\"fact_sales:\")\n",
    "print(df_fact.head())\n",
    "\n",
    "# 3️⃣ Load transformed data into PostgreSQL\n",
    "load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact)\n",
    "print(\"\\n✅ ETL process completed for 2025-01-02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69abe26d-2305-4889-9742-903876ef7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "import logging\n",
    "\n",
    "# Set up logging for visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Database Connection Configurations\n",
    "# =========================================================\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"@Whatsnew2711\"\n",
    "MYSQL_DB = \"coffee_shop_sales_oltp\"\n",
    "\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"Whatsnew2711\"\n",
    "PG_DB = \"Coffee_Machine_Project_One\"\n",
    "\n",
    "def get_postgres_connection():\n",
    "    \"\"\"Establishes and returns a PostgreSQL database connection.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD,\n",
    "            dbname=PG_DB\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ ETL Functions\n",
    "# =========================================================\n",
    "\n",
    "def extract_data_from_mysql(date_to_process):\n",
    "    \"\"\"\n",
    "    Extracts daily transaction data from MySQL OLTP database\n",
    "    for a specific date.\n",
    "    \"\"\"\n",
    "    logging.info(f\"📥 Extracting data from MySQL for {date_to_process}...\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            port=MYSQL_PORT,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            transaction_date,\n",
    "            transaction_time,\n",
    "            transaction_qty,\n",
    "            store_id,\n",
    "            store_location,\n",
    "            product_id,\n",
    "            unit_price,\n",
    "            product_category,\n",
    "            product_type,\n",
    "            product_detail\n",
    "        FROM sales_data\n",
    "        WHERE transaction_date = '{date_to_process}';\n",
    "        \"\"\"\n",
    "        df_raw = pd.read_sql(query, conn)\n",
    "        logging.info(f\"✅ Extracted {len(df_raw)} records.\")\n",
    "        return df_raw\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"❌ MySQL Error: {err}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"MySQL connection closed.\")\n",
    "\n",
    "# =============================\n",
    "# Transform Data (Postgres-ready)\n",
    "# =============================\n",
    "\n",
    "def transform_data(df_raw):\n",
    "    \"\"\"\n",
    "    Transforms raw MySQL data into star schema DataFrames that match PostgreSQL tables.\n",
    "    Returns: dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    logging.info(\"🔄 Transforming data for OLAP...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_date to datetime\n",
    "    # ----------------------------\n",
    "    if \"transaction_date\" in df_raw.columns:\n",
    "        df_raw[\"transaction_date\"] = pd.to_datetime(df_raw[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert transaction_time from timedelta to time\n",
    "    # ----------------------------\n",
    "    if \"transaction_time\" in df_raw.columns:\n",
    "        if pd.api.types.is_timedelta64_dtype(df_raw[\"transaction_time\"]):\n",
    "            df_raw[\"transaction_time\"] = df_raw[\"transaction_time\"].apply(\n",
    "                lambda x: (pd.Timestamp(\"00:00:00\") + x).time() if pd.notnull(x) else None\n",
    "            )\n",
    "        else:\n",
    "            df_raw[\"transaction_time\"] = pd.to_datetime(\n",
    "                df_raw[\"transaction_time\"].astype(str), errors=\"coerce\"\n",
    "            ).dt.time\n",
    "\n",
    "    # Drop rows where conversion failed\n",
    "    df_raw = df_raw.dropna(subset=[\"transaction_date\", \"transaction_time\"])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Calculate total_amount\n",
    "    # ----------------------------\n",
    "    df_raw[\"total_amount\"] = df_raw[\"transaction_qty\"] * df_raw[\"unit_price\"]\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Date\n",
    "    # ----------------------------\n",
    "    dim_date = df_raw[[\"transaction_date\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_date[\"day\"] = dim_date[\"transaction_date\"].dt.day\n",
    "    dim_date[\"month\"] = dim_date[\"transaction_date\"].dt.month\n",
    "    dim_date[\"year\"] = dim_date[\"transaction_date\"].dt.year\n",
    "    dim_date[\"weekday\"] = dim_date[\"transaction_date\"].dt.day_name()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Time\n",
    "    # ----------------------------\n",
    "    dim_time = df_raw[[\"transaction_time\"]].drop_duplicates().reset_index(drop=True)\n",
    "    dim_time[\"hour\"] = dim_time[\"transaction_time\"].apply(lambda x: x.hour)\n",
    "    dim_time[\"minute\"] = dim_time[\"transaction_time\"].apply(lambda x: x.minute)\n",
    "    dim_time[\"second\"] = dim_time[\"transaction_time\"].apply(lambda x: x.second)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Store\n",
    "    # ----------------------------\n",
    "    dim_store = df_raw[[\"store_id\", \"store_location\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dim Product\n",
    "    # ----------------------------\n",
    "    dim_product = df_raw[[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Fact Table\n",
    "    # ----------------------------\n",
    "    df_fact = df_raw.copy()\n",
    "    df_fact = df_fact.merge(dim_date, on=\"transaction_date\")\n",
    "    df_fact = df_fact.merge(dim_time, on=\"transaction_time\")\n",
    "    df_fact = df_fact.merge(dim_store, on=[\"store_id\", \"store_location\"])\n",
    "    df_fact = df_fact.merge(dim_product, on=[\"product_id\", \"product_category\", \"product_type\", \"product_detail\", \"unit_price\"])\n",
    "\n",
    "    df_fact = df_fact[[\n",
    "        \"transaction_id\",\n",
    "        \"transaction_date\",  # maps to dim_date.date_id in load\n",
    "        \"transaction_time\",  # maps to dim_time.time_id\n",
    "        \"store_id\",          # maps to dim_store.store_id\n",
    "        \"product_id\",        # maps to dim_product.product_id\n",
    "        \"transaction_qty\",\n",
    "        \"total_amount\"\n",
    "    ]]\n",
    "\n",
    "    logging.info(f\"✅ Transformation completed. Records: {len(df_fact)}\")\n",
    "    return dim_date, dim_time, dim_store, dim_product, df_fact\n",
    "\n",
    "# =============================\n",
    "# Load Data (Corrected)\n",
    "# =============================\n",
    "\n",
    "def load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact):\n",
    "    \"\"\"\n",
    "    Loads transformed data into PostgreSQL OLAP database\n",
    "    by checking for existence before inserting to avoid duplicates.\n",
    "    \"\"\"\n",
    "    logging.info(\"📤 Loading data to PostgreSQL...\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_postgres_connection()\n",
    "        if not conn:\n",
    "            return\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Date (Check then Insert)\n",
    "        # -----------------------------\n",
    "        date_id_map = {}\n",
    "        for _, row in dim_date.iterrows():\n",
    "            cur.execute(\"SELECT date_id FROM dim_date WHERE transaction_date = %s;\", (row[\"transaction_date\"].date(),))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                date_id_map[row[\"transaction_date\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_date (transaction_date, day, month, year, weekday)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                    RETURNING date_id;\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_date\"].date(),\n",
    "                    int(row[\"day\"]),\n",
    "                    int(row[\"month\"]),\n",
    "                    int(row[\"year\"]),\n",
    "                    row[\"weekday\"]\n",
    "                ))\n",
    "                date_id_map[row[\"transaction_date\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Time (Check then Insert)\n",
    "        # -----------------------------\n",
    "        time_id_map = {}\n",
    "        for _, row in dim_time.iterrows():\n",
    "            cur.execute(\"SELECT time_id FROM dim_time WHERE transaction_time = %s;\", (row[\"transaction_time\"],))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                time_id_map[row[\"transaction_time\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_time (transaction_time, hour, minute, second)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING time_id;\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_time\"],\n",
    "                    int(row[\"hour\"]),\n",
    "                    int(row[\"minute\"]),\n",
    "                    int(row[\"second\"])\n",
    "                ))\n",
    "                time_id_map[row[\"transaction_time\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Store (Check then Insert)\n",
    "        # -----------------------------\n",
    "        store_id_map = {}\n",
    "        for _, row in dim_store.iterrows():\n",
    "            cur.execute(\"SELECT store_id FROM dim_store WHERE store_location = %s;\", (row[\"store_location\"],))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                store_id_map[row[\"store_id\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_store (store_location)\n",
    "                    VALUES (%s)\n",
    "                    RETURNING store_id;\n",
    "                \"\"\", (row[\"store_location\"],))\n",
    "                store_id_map[row[\"store_id\"]] = cur.fetchone()[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dim Product (Check then Insert)\n",
    "        # -----------------------------\n",
    "        product_id_map = {}\n",
    "        for _, row in dim_product.iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT product_id FROM dim_product\n",
    "                WHERE product_category = %s AND product_type = %s AND product_detail = %s;\n",
    "            \"\"\", (row[\"product_category\"], row[\"product_type\"], row[\"product_detail\"]))\n",
    "            result = cur.fetchone()\n",
    "\n",
    "            if result:\n",
    "                product_id_map[row[\"product_id\"]] = result[0]\n",
    "            else:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO dim_product (product_category, product_type, product_detail, unit_price)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING product_id;\n",
    "                \"\"\", (\n",
    "                    row[\"product_category\"],\n",
    "                    row[\"product_type\"],\n",
    "                    row[\"product_detail\"],\n",
    "                    float(row[\"unit_price\"])\n",
    "                ))\n",
    "                product_id_map[row[\"product_id\"]] = cur.fetchone()[0]\n",
    "        \n",
    "        conn.commit()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fact Table (Check then Insert)\n",
    "        # -----------------------------\n",
    "        for _, row in df_fact.iterrows():\n",
    "            cur.execute(\"SELECT transaction_id FROM fact_sales WHERE transaction_id = %s;\", (row[\"transaction_id\"],))\n",
    "            result = cur.fetchone()\n",
    "            \n",
    "            if not result:\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO fact_sales (transaction_id, date_id, time_id, store_id, product_id, transaction_qty, total_amount)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\"\", (\n",
    "                    row[\"transaction_id\"],\n",
    "                    date_id_map[row[\"transaction_date\"]],\n",
    "                    time_id_map[row[\"transaction_time\"]],\n",
    "                    store_id_map[row[\"store_id\"]],\n",
    "                    product_id_map[row[\"product_id\"]],\n",
    "                    int(row[\"transaction_qty\"]),\n",
    "                    float(row[\"total_amount\"])\n",
    "                ))\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        logging.info(\"✅ Data loaded to PostgreSQL successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to load data to PostgreSQL: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "575c155a-0db6-4516-994c-a84c162170b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 09:26:54,457 - INFO - 🚀 Starting daily incremental ETL process...\n",
      "2025-08-20 09:26:54,460 - INFO - 🗓️ Checking for the last processed date in PostgreSQL...\n",
      "/var/folders/jh/nl0hs4bx26l4cbrp239k3n9c0000gn/T/ipykernel_1405/246310345.py:62: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  last_date_df = pd.read_sql_query(text(query), conn)\n",
      "2025-08-20 09:26:54,574 - ERROR - ❌ Error in get_next_date_to_process: Query must be a string unless using sqlalchemy.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/jh/nl0hs4bx26l4cbrp239k3n9c0000gn/T/ipykernel_1405/246310345.py\", line 62, in get_next_date_to_process\n",
      "    last_date_df = pd.read_sql_query(text(query), conn)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages/pandas/io/sql.py\", line 528, in read_sql_query\n",
      "    return pandas_sql.read_query(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages/pandas/io/sql.py\", line 2728, in read_query\n",
      "    cursor = self.execute(sql, params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages/pandas/io/sql.py\", line 2660, in execute\n",
      "    raise TypeError(\"Query must be a string unless using sqlalchemy.\")\n",
      "TypeError: Query must be a string unless using sqlalchemy.\n",
      "2025-08-20 09:26:54,575 - ERROR - ❌ Failed to determine the next date to process. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import sys\n",
    "import mysql.connector\n",
    "\n",
    "# Import the ETL functions from your main script\n",
    "from etl_olap_postgresql import (\n",
    "    extract_data_from_mysql,\n",
    "    transform_data,\n",
    "    load_data_to_postgres,\n",
    "    get_postgres_connection\n",
    ")\n",
    "\n",
    "# =========================================================================\n",
    "# Logging Configuration\n",
    "# =========================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# =========================================================================\n",
    "# Database Configurations for the Scheduler\n",
    "# =========================================================================\n",
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_PORT = 3306\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASSWORD = '@Whatsnew2711'\n",
    "MYSQL_DB = 'coffee_shop_sales_oltp'\n",
    "\n",
    "PG_HOST = 'localhost'\n",
    "PG_PORT = 5432\n",
    "PG_USER = 'postgres'\n",
    "PG_PASSWORD = 'Whatsnew2711'\n",
    "PG_DB = 'Coffee_Machine_Project_One'\n",
    "\n",
    "try:\n",
    "    mysql_conn_str = f\"mysql+mysqlconnector://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}\"\n",
    "    pg_conn_str = f\"postgresql+psycopg2://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "\n",
    "    mysql_engine = create_engine(mysql_conn_str)\n",
    "    pg_engine = create_engine(pg_conn_str)\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ Failed to create database engines for scheduler: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================================\n",
    "# Scheduler Functions\n",
    "# =========================================================================\n",
    "\n",
    "def get_next_date_to_process():\n",
    "    \"\"\"\n",
    "    Determine the next date to process by finding the last date\n",
    "    in the PostgreSQL OLAP fact table.\n",
    "    \"\"\"\n",
    "    logging.info(\"🗓️ Checking for the last processed date in PostgreSQL...\")\n",
    "    try:\n",
    "        query = \"SELECT MAX(transaction_date) FROM fact_sales;\"\n",
    "        with pg_engine.connect() as conn:\n",
    "            last_date_df = pd.read_sql_query(text(query), conn)\n",
    "            last_date = last_date_df.iloc[0, 0]\n",
    "\n",
    "        if last_date is None or pd.isna(last_date):\n",
    "            logging.info(\"⭐ OLAP database is empty. Finding earliest date from MySQL...\")\n",
    "            query = \"SELECT MIN(transaction_date) FROM sales_data;\"\n",
    "            with mysql_engine.connect() as conn:\n",
    "                earliest_date_df = pd.read_sql_query(text(query), conn)\n",
    "                earliest_date = earliest_date_df.iloc[0, 0]\n",
    "\n",
    "            if earliest_date is None or pd.isna(earliest_date):\n",
    "                logging.warning(\"⚠️ No data found in the source MySQL database.\")\n",
    "                return None\n",
    "            return pd.to_datetime(earliest_date)\n",
    "        else:\n",
    "            return pd.to_datetime(last_date) + timedelta(days=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Error in get_next_date_to_process: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_daily_incremental():\n",
    "    \"\"\"\n",
    "    Orchestrates the full ETL process from MySQL to PostgreSQL for a single day.\n",
    "    \"\"\"\n",
    "    logging.info(\"🚀 Starting daily incremental ETL process...\")\n",
    "\n",
    "    next_date = get_next_date_to_process()\n",
    "    if next_date is None:\n",
    "        logging.error(\"❌ Failed to determine the next date to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # ✅ Use SQLAlchemy connection for MySQL query\n",
    "    query = \"SELECT MAX(transaction_date) FROM sales_data;\"\n",
    "    with mysql_engine.connect() as conn:\n",
    "        last_raw_date_df = pd.read_sql_query(text(query), conn)\n",
    "        last_raw_date = last_raw_date_df.iloc[0, 0]\n",
    "\n",
    "    if pd.isna(last_raw_date) or next_date.date() > pd.to_datetime(last_raw_date).date():\n",
    "        logging.info(\"✅ No new data to process. All available data is loaded.\")\n",
    "        return\n",
    "\n",
    "    date_to_process = next_date.strftime('%Y-%m-%d')\n",
    "    logging.info(f\"➡️ Processing daily incremental ETL for: {date_to_process}\")\n",
    "\n",
    "    df_raw = extract_data_from_mysql(date_to_process)\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        logging.warning(f\"⚠️ No data found for {date_to_process}. Skipping load.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        dim_date, dim_time, dim_store, dim_product, df_fact = transform_data(df_raw)\n",
    "        load_data_to_postgres(dim_date, dim_time, dim_store, dim_product, df_fact)\n",
    "        logging.info(f\"✅ Daily incremental ETL for {date_to_process} completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ An error occurred during the ETL process for {date_to_process}: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_daily_incremental()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42e42a74-5ab7-4bf4-a935-5015d423d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in /Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages (9.4.0)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-macosx_12_0_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: sqlalchemy in /Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages (1.4.49)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mac/anaconda3/envs/airflow_env/lib/python3.11/site-packages (from sqlalchemy) (2.0.2)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-macosx_12_0_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0965430e-f59f-406f-84df-3d737899f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter start date (YYYY-MM-DD):  2025-01-01\n",
      "Enter end date (YYYY-MM-DD):  2025-05-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Extracting data from 'Coffee Shop Sales2.csv'...\n",
      "🔄 Transforming data...\n",
      "🆕 No existing data. Creating a new file...\n",
      "✅ Load complete! Total unique transactions: 85474\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_data(input_csv):\n",
    "    \"\"\"\n",
    "    Extracts data from a specified CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"📥 Extracting data from '{input_csv}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, sep=\";\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The file '{input_csv}' was not found.\")\n",
    "        return None\n",
    "\n",
    "def transform_data(df, start_offset_date, end_offset_date):\n",
    "    \"\"\"\n",
    "    Transforms the raw DataFrame by:\n",
    "    - Filtering by date range\n",
    "    - Formatting dates & times\n",
    "    - Mapping store locations\n",
    "    - Converting currency\n",
    "    - Regenerating unique transaction IDs\n",
    "    \"\"\"\n",
    "    print(\"🔄 Transforming data...\")\n",
    "\n",
    "    # Convert date and time\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "    df['transaction_time'] = pd.to_datetime(\n",
    "        df['transaction_time'], format='%H:%M:%S', errors='coerce'\n",
    "    ).dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Filter by date range\n",
    "    start_date = pd.to_datetime(start_offset_date)\n",
    "    end_date = pd.to_datetime(end_offset_date)\n",
    "    df_filtered = df[\n",
    "        (df['transaction_date'] >= start_date) & (df['transaction_date'] <= end_date)\n",
    "    ].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"⚠ No data found for the range {start_offset_date} to {end_offset_date}.\")\n",
    "        return df_filtered\n",
    "\n",
    "    # Format date for output\n",
    "    df_filtered['transaction_date'] = df_filtered['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Map store locations\n",
    "    location_map = {\n",
    "        \"Hell's Kitchen\": \"East Campus\",\n",
    "        \"Astoria\": \"West Campus\",\n",
    "        \"Lower Manhattan\": \"Main Campus\"\n",
    "    }\n",
    "    df_filtered['store_location'] = df_filtered['store_location'].replace(location_map)\n",
    "\n",
    "    # Convert unit_price\n",
    "    df_filtered['unit_price'] = (df_filtered['unit_price'] * 15).round(2)\n",
    "\n",
    "    # Convert key columns to string safely\n",
    "    cols_to_convert = ['transaction_id', 'store_id', 'product_id']\n",
    "    df_filtered[cols_to_convert] = df_filtered[cols_to_convert].apply(\n",
    "        lambda col: col.dropna().astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # Force unique transaction IDs by appending the date\n",
    "    df_filtered['transaction_id'] = (\n",
    "        df_filtered['transaction_id'].astype(str) + \"_\" + \n",
    "        pd.to_datetime(df_filtered['transaction_date']).dt.strftime('%Y%m%d')\n",
    "    )\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def load_data(df_transformed, output_csv, start_offset_date, end_offset_date):\n",
    "    \"\"\"\n",
    "    Loads transformed data into the CSV, replacing rows in the given date range\n",
    "    and ensuring all transaction IDs are unique across the file.\n",
    "    \"\"\"\n",
    "    if df_transformed.empty:\n",
    "        print(\"⚠ No data to load.\")\n",
    "        return\n",
    "\n",
    "    # Ensure proper types\n",
    "    df_transformed['transaction_date'] = pd.to_datetime(df_transformed['transaction_date'])\n",
    "    df_transformed['transaction_id'] = df_transformed['transaction_id'].astype(str)\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        print(\"📂 Existing data found. Removing old data in the same date range...\")\n",
    "        df_existing = pd.read_csv(output_csv)\n",
    "        df_existing['transaction_date'] = pd.to_datetime(df_existing['transaction_date'])\n",
    "        df_existing['transaction_id'] = df_existing['transaction_id'].astype(str)\n",
    "\n",
    "        # Remove rows from existing file that fall within the same date range\n",
    "        mask = ~(\n",
    "            (df_existing['transaction_date'] >= pd.to_datetime(start_offset_date)) &\n",
    "            (df_existing['transaction_date'] <= pd.to_datetime(end_offset_date))\n",
    "        )\n",
    "        df_existing = df_existing[mask]\n",
    "\n",
    "        # Append new transformed data\n",
    "        df_final = pd.concat([df_existing, df_transformed], ignore_index=True)\n",
    "    else:\n",
    "        print(\"🆕 No existing data. Creating a new file...\")\n",
    "        df_final = df_transformed.copy()\n",
    "\n",
    "    # Keep only unique transaction IDs (final safeguard)\n",
    "    df_final = df_final.drop_duplicates(subset=['transaction_id'], keep='last')\n",
    "    df_final = df_final.sort_values(by=['transaction_date', 'transaction_time'])\n",
    "\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Load complete! Total unique transactions: {df_final['transaction_id'].nunique()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"Coffee Shop Sales2.csv\"  # your raw CSV file\n",
    "    output_file = \"processed_sales_data.csv\"  # final output\n",
    "\n",
    "    start_date = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
    "    end_date = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
    "\n",
    "    df_raw = extract_data(input_file)\n",
    "    if df_raw is not None:\n",
    "        df_transformed = transform_data(df_raw, start_date, end_date)\n",
    "        load_data(df_transformed, output_file, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5a526-09dd-4ff2-a6b5-db4786d25161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
