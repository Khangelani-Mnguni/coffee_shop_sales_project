{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0d39c31-76ae-487f-9400-08224cceaf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Extracting data from 'Coffee Shop Sales2.csv'...\n",
      "üìÜ Processing daily backdate for: 2025-01-01\n",
      "üîÑ Transforming data...\n",
      "üÜï No existing data. Creating a new file...\n",
      "‚úÖ Load complete! Total unique transactions: 549\n",
      "‚úÖ Daily backdate for 2025-01-01 completed.\n"
     ]
    }
   ],
   "source": [
    "import development.scheduled_csv_to_mysql\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_daily_incremental()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63df71ad-d809-4ec4-bfc0-f8ba51a389c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2856326376.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[48], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    install pandas\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78ebc3da-d748-4549-9c26-12436acf4d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\khang\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\khang\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\khang\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\khang\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\khang\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\khang\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc2b5e0-1150-4674-8fa4-7b83abe4ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from etl_backdating_csv_to_csv import extract_data, transform_data, load_data\n",
    "\n",
    "INPUT_FILE = \"Coffee Shop Sales2.csv\"\n",
    "OUTPUT_FILE = \"transformed.csv\"\n",
    "\n",
    "def get_next_date_to_process(output_csv, df_raw):\n",
    "    \"\"\"\n",
    "    Determine the next date to backdate based on the last date in the CSV.\n",
    "    If the CSV doesn't exist, return the earliest date in the raw data.\n",
    "    \"\"\"\n",
    "    df_raw['transaction_date'] = pd.to_datetime(df_raw['transaction_date'], errors='coerce')\n",
    "    \n",
    "    if not os.path.exists(output_csv):\n",
    "        return df_raw['transaction_date'].min()\n",
    "\n",
    "    df_existing = pd.read_csv(output_csv)\n",
    "    df_existing['transaction_date'] = pd.to_datetime(df_existing['transaction_date'], errors='coerce')\n",
    "    df_existing = df_existing.dropna(subset=['transaction_date'])\n",
    "    last_date = df_existing['transaction_date'].max()\n",
    "\n",
    "    return last_date + timedelta(days=1)\n",
    "\n",
    "\n",
    "def run_daily_incremental():\n",
    "    \"\"\"\n",
    "    Run ETL for the next unprocessed date in the raw CSV.\n",
    "    \"\"\"\n",
    "    df_raw = extract_data(INPUT_FILE)\n",
    "    if df_raw is None:\n",
    "        print(\"‚ùå Extraction failed.\")\n",
    "        return\n",
    "\n",
    "    next_date = get_next_date_to_process(OUTPUT_FILE, df_raw)\n",
    "\n",
    "    df_raw['transaction_date'] = pd.to_datetime(df_raw['transaction_date'], errors='coerce')\n",
    "    df_raw = df_raw.dropna(subset=['transaction_date'])\n",
    "\n",
    "    last_raw_date = df_raw['transaction_date'].max()\n",
    "    if next_date > last_raw_date:\n",
    "        print(\"‚úÖ No new data to process. All dates up to latest available are loaded.\")\n",
    "        return\n",
    "\n",
    "    start_date_str = next_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = next_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"üìÜ Processing daily backdate for: {start_date_str}\")\n",
    "\n",
    "    # --- Transform and load ---\n",
    "    df_transformed = transform_data(df_raw, start_date_str, end_date_str)\n",
    "    if df_transformed.empty:\n",
    "        print(f\"‚ö† No data found for {start_date_str}\")\n",
    "        return\n",
    "\n",
    "    load_data(df_transformed, OUTPUT_FILE, start_date_str, end_date_str)\n",
    "    print(f\"‚úÖ Daily backdate for {start_date_str} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b617f1e-7992-4bf6-9cb7-7e646aa387b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Extracting data from 'Coffee Shop Sales2.csv'...\n",
      "üìÜ Processing daily backdate for: 2025-01-02\n",
      "üîÑ Transforming data...\n",
      "üìÇ Existing data found. Removing old data in the same date range...\n",
      "‚úÖ Load complete! Total unique transactions: 1115\n",
      "‚úÖ Daily backdate for 2025-01-02 completed.\n"
     ]
    }
   ],
   "source": [
    "run_daily_incremental()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae0f41b-36c4-45ec-9d8c-889e6e8df05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"‚úÖ Data extracted successfully from {file_path}\")\n",
    "        print(\"Columns in CSV:\", df.columns.tolist())  # üîç Add this line\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during data extraction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02974566-8cd1-400d-9267-957eb04ca4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"Coffee Shop Sales2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca0060d5-7999-4984-83cc-5f2eb342eabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data extracted successfully from Coffee Shop Sales2.csv\n",
      "Columns in CSV: ['transaction_id;transaction_date;transaction_time;transaction_qty;store_id;store_location;product_id;unit_price;product_category;product_type;product_detail']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id;transaction_date;transaction_time;transaction_qty;store_id;store_location;product_id;unit_price;product_category;product_type;product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1;2025/01/01;07:06:11;2;5;Lower Manhattan;32;3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2;2025/01/01;07:08:56;2;5;Lower Manhattan;57;3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3;2025/01/01;07:14:04;2;5;Lower Manhattan;59;4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4;2025/01/01;07:20:24;1;5;Lower Manhattan;22;2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5;2025/01/01;07:22:41;2;5;Lower Manhattan;57;3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149111</th>\n",
       "      <td>149452;2025/06/30;20:18:41;2;8;Hell's Kitchen;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149112</th>\n",
       "      <td>149453;2025/06/30;20:25:10;2;8;Hell's Kitchen;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149113</th>\n",
       "      <td>149454;2025/06/30;20:31:34;1;8;Hell's Kitchen;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149114</th>\n",
       "      <td>149455;2025/06/30;20:57:19;1;8;Hell's Kitchen;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149115</th>\n",
       "      <td>149456;2025/06/30;20:57:19;2;8;Hell's Kitchen;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149116 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       transaction_id;transaction_date;transaction_time;transaction_qty;store_id;store_location;product_id;unit_price;product_category;product_type;product_detail\n",
       "0       1;2025/01/01;07:06:11;2;5;Lower Manhattan;32;3...                                                                                                         \n",
       "1       2;2025/01/01;07:08:56;2;5;Lower Manhattan;57;3...                                                                                                         \n",
       "2       3;2025/01/01;07:14:04;2;5;Lower Manhattan;59;4...                                                                                                         \n",
       "3       4;2025/01/01;07:20:24;1;5;Lower Manhattan;22;2...                                                                                                         \n",
       "4       5;2025/01/01;07:22:41;2;5;Lower Manhattan;57;3...                                                                                                         \n",
       "...                                                   ...                                                                                                         \n",
       "149111  149452;2025/06/30;20:18:41;2;8;Hell's Kitchen;...                                                                                                         \n",
       "149112  149453;2025/06/30;20:25:10;2;8;Hell's Kitchen;...                                                                                                         \n",
       "149113  149454;2025/06/30;20:31:34;1;8;Hell's Kitchen;...                                                                                                         \n",
       "149114  149455;2025/06/30;20:57:19;1;8;Hell's Kitchen;...                                                                                                         \n",
       "149115  149456;2025/06/30;20:57:19;2;8;Hell's Kitchen;...                                                                                                         \n",
       "\n",
       "[149116 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_data(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a184278-73ab-46d0-9158-1f03dc10db92",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'exec'. Did you mean exec(...)? (2902784441.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    docker exec -it airflow-docker-airflow-scheduler-1 bash\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'exec'. Did you mean exec(...)?\n"
     ]
    }
   ],
   "source": [
    "docker exec -it airflow-docker-airflow-scheduler-1 bash\n",
    "python3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb4642a-7247-495a-9e95-63ddc8e7f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-37.5.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tzdata in c:\\users\\khang\\anaconda3\\lib\\site-packages (from faker) (2025.2)\n",
      "Downloading faker-37.5.3-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.9 MB 435.7 kB/s eta 0:00:05\n",
      "    --------------------------------------- 0.0/1.9 MB 435.7 kB/s eta 0:00:05\n",
      "    --------------------------------------- 0.0/1.9 MB 435.7 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/1.9 MB 252.2 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/1.9 MB 327.7 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.9 MB 327.7 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.9 MB 327.7 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.9 MB 300.4 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.2/1.9 MB 436.8 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 492.1 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 492.1 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.2/1.9 MB 430.6 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.4/1.9 MB 603.4 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.5/1.9 MB 718.9 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 733.7 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 683.6 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 696.3 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 696.3 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 696.3 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 696.3 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 579.2 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 579.2 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 579.2 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 579.2 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 579.2 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 500.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/1.9 MB 450.6 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 580.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 580.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 580.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 551.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 567.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 567.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 567.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 567.3 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.2/1.9 MB 543.2 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.3/1.9 MB 567.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 594.7 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/1.9 MB 617.1 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 618.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 639.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 667.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 679.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 690.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 708.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 711.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 693.2 kB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-37.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d84c17-3205-4665-bdee-2f3007627e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import mysql.connector\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "\n",
    "# Set up logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "fake = Faker()\n",
    "\n",
    "def generate_synthetic_data(start_date, end_date, num_records_per_day):\n",
    "    \"\"\"\n",
    "    Generates synthetic coffee shop sales data within a specified date range.\n",
    "    \"\"\"\n",
    "    print(\"‚ú® Generating synthetic data...\")\n",
    "    \n",
    "    # Define product categories and their corresponding types and details\n",
    "    product_data = {\n",
    "        'Coffee': {\n",
    "            'Gourmet brewed coffee': ['Ethiopia Rg', 'Colombian Sm', 'Brazilian Lg'],\n",
    "            'Drip coffee': ['Our Old Time Diner Blend Sm', 'Our Old Time Diner Blend Lg'],\n",
    "            'Organic brewed coffee': ['Brazilian Rg', 'Peruvian Lg'],\n",
    "            'Premium brewed coffee': ['Jamaican Coffee River Sm', 'Blue Mountain Rg'],\n",
    "            'Espresso': ['Americano Rg', 'Latte Lg', 'Cappuccino Sm']\n",
    "        },\n",
    "        'Tea': {\n",
    "            'Brewed Chai tea': ['Spicy Eye Opener Chai Lg', 'Morning Sunrise Chai Lg'],\n",
    "            'Brewed Black tea': ['Earl Grey Rg', 'English Breakfast Sm'],\n",
    "            'Brewed Herbal tea': ['Chamomile Lg', 'Peppermint Rg']\n",
    "        },\n",
    "        'Drinking Chocolate': {\n",
    "            'Hot chocolate': ['Dark chocolate Lg', 'Caramel Lg'],\n",
    "            'Flavored hot chocolate': ['White chocolate Sm', 'Hazelnut Rg']\n",
    "        },\n",
    "        'Bakery': {\n",
    "            'Scone': ['Oatmeal Scone', 'Cranberry Scone'],\n",
    "            'Pastry': ['Croissant', 'Apple Danish'],\n",
    "            'Muffin': ['Blueberry Muffin', 'Chocolate Chip Muffin']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define store locations and their IDs\n",
    "    store_locations = {\n",
    "        'Lower Manhattan': 5,\n",
    "        'Astoria': 3,\n",
    "        \"Hell's Kitchen\": 8\n",
    "    }\n",
    "\n",
    "    # Generate a list of dates\n",
    "    date_list = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "    \n",
    "    records = []\n",
    "    transaction_id_counter = 1\n",
    "\n",
    "    for date in date_list:\n",
    "        # Simulate varying number of transactions per day\n",
    "        num_transactions = random.randint(num_records_per_day // 2, num_records_per_day * 2)\n",
    "        \n",
    "        for _ in range(num_transactions):\n",
    "            transaction_time = fake.time_object().strftime('%H:%M:%S')\n",
    "            transaction_qty = random.randint(1, 3)\n",
    "            store_location = random.choice(list(store_locations.keys()))\n",
    "            store_id = store_locations[store_location]\n",
    "            \n",
    "            product_category = random.choice(list(product_data.keys()))\n",
    "            product_type = random.choice(list(product_data[product_category].keys()))\n",
    "            product_detail = random.choice(product_data[product_category][product_type])\n",
    "            \n",
    "            # Unit price based on product category and size\n",
    "            if 'Sm' in product_detail:\n",
    "                unit_price = round(random.uniform(2.0, 3.0), 2)\n",
    "            elif 'Rg' in product_detail:\n",
    "                unit_price = round(random.uniform(2.5, 4.0), 2)\n",
    "            elif 'Lg' in product_detail:\n",
    "                unit_price = round(random.uniform(3.0, 5.0), 2)\n",
    "            else:\n",
    "                unit_price = round(random.uniform(2.0, 5.0), 2)\n",
    "\n",
    "            # Assign product ID\n",
    "            product_id = random.randint(1, 100)\n",
    "            \n",
    "            records.append([\n",
    "                transaction_id_counter,\n",
    "                date.strftime('%Y-%m-%d'),\n",
    "                transaction_time,\n",
    "                transaction_qty,\n",
    "                store_id,\n",
    "                store_location,\n",
    "                product_id,\n",
    "                unit_price,\n",
    "                product_category,\n",
    "                product_type,\n",
    "                product_detail\n",
    "            ])\n",
    "            transaction_id_counter += 1\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\n",
    "        'transaction_id', 'transaction_date', 'transaction_time', 'transaction_qty',\n",
    "        'store_id', 'store_location', 'product_id', 'unit_price',\n",
    "        'product_category', 'product_type', 'product_detail'\n",
    "    ])\n",
    "\n",
    "    print(f\"‚úÖ Data generation complete! Generated {len(df)} records.\")\n",
    "    return df\n",
    "\n",
    "def transform_data(df, start_offset_date, end_offset_date):\n",
    "    \"\"\"\n",
    "    Transforms the raw DataFrame by:\n",
    "    - Filtering by date range\n",
    "    - Formatting dates & times\n",
    "    - Mapping store locations\n",
    "    - Converting currency\n",
    "    - Regenerating unique transaction IDs\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Transforming data...\")\n",
    "\n",
    "    # Convert date and time\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "    df['transaction_time'] = pd.to_datetime(\n",
    "        df['transaction_time'], format='%H:%M:%S', errors='coerce'\n",
    "    ).dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Filter by date range\n",
    "    start_date = pd.to_datetime(start_offset_date)\n",
    "    end_date = pd.to_datetime(end_offset_date)\n",
    "    df_filtered = df[\n",
    "        (df['transaction_date'] >= start_date) & (df['transaction_date'] <= end_date)\n",
    "    ].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"‚ö† No data found for the range {start_offset_date} to {end_offset_date}.\")\n",
    "        return df_filtered\n",
    "\n",
    "    # Format date for output\n",
    "    df_filtered['transaction_date'] = df_filtered['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Map store locations\n",
    "    location_map = {\n",
    "        \"Hell's Kitchen\": \"East Campus\",\n",
    "        \"Astoria\": \"West Campus\",\n",
    "        \"Lower Manhattan\": \"Main Campus\"\n",
    "    }\n",
    "    df_filtered['store_location'] = df_filtered['store_location'].replace(location_map)\n",
    "\n",
    "    # Convert unit_price\n",
    "    df_filtered['unit_price'] = (df_filtered['unit_price'] * 15).round(2)\n",
    "\n",
    "    # Convert key columns to string safely\n",
    "    cols_to_convert = ['transaction_id', 'store_id', 'product_id']\n",
    "    df_filtered[cols_to_convert] = df_filtered[cols_to_convert].apply(\n",
    "        lambda col: col.dropna().astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # Force unique transaction IDs by appending the date\n",
    "    df_filtered['transaction_id'] = (\n",
    "        df_filtered['transaction_id'].astype(str) + \"_\" +\n",
    "        pd.to_datetime(df_filtered['transaction_date']).dt.strftime('%Y%m%d')\n",
    "    )\n",
    "    \n",
    "    # Reorder columns to match the MySQL table structure\n",
    "    df_transformed = df_filtered.reindex(columns=[\n",
    "        'transaction_id', 'transaction_date', 'transaction_time', 'transaction_qty',\n",
    "        'store_id', 'store_location', 'product_id', 'unit_price',\n",
    "        'product_category', 'product_type', 'product_detail'\n",
    "    ], fill_value=None)\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def load_data_to_mysql(df, db_config, table_name, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Loads transformed data into a MySQL table in batches.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ö† No data to load to MySQL.\")\n",
    "        return\n",
    "\n",
    "    print(\"üöÄ Loading data to MySQL...\")\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Successfully connected to MySQL database.\")\n",
    "\n",
    "        records = [tuple(x) for x in df.to_numpy()]\n",
    "        num_records = len(records)\n",
    "\n",
    "        # The INSERT query with ON DUPLICATE KEY UPDATE to handle idempotency\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} (\n",
    "                transaction_id, transaction_date, transaction_time, transaction_qty,\n",
    "                store_id, store_location, product_id, unit_price,\n",
    "                product_category, product_type, product_detail\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                transaction_date=VALUES(transaction_date),\n",
    "                transaction_time=VALUES(transaction_time),\n",
    "                transaction_qty=VALUES(transaction_qty),\n",
    "                store_id=VALUES(store_id),\n",
    "                store_location=VALUES(store_location),\n",
    "                product_id=VALUES(product_id),\n",
    "                unit_price=VALUES(unit_price),\n",
    "                product_category=VALUES(product_category),\n",
    "                product_type=VALUES(product_type),\n",
    "                product_detail=VALUES(product_detail)\n",
    "        \"\"\"\n",
    "\n",
    "        # Batch loading\n",
    "        for i in range(0, num_records, batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            print(f\"üì¶ Loading batch {i // batch_size + 1} of {num_records // batch_size + 1}...\")\n",
    "            cursor.executemany(insert_query, batch)\n",
    "            conn.commit()\n",
    "\n",
    "        print(f\"‚úÖ Load complete! Inserted/updated {num_records} records.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        logging.error(f\"‚ùå MySQL Error: {err}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor is not None:\n",
    "            cursor.close()\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.close()\n",
    "        print(\"MySQL connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    table_name = \"sales_data\"\n",
    "    num_records_per_day = 500  # Adjust to generate more or fewer records\n",
    "    \n",
    "    db_config = {\n",
    "        'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': 'Whatsnew2711',\n",
    "        'database': 'coffee_shop_sales'\n",
    "    }\n",
    "\n",
    "    start_date_str = input(\"Enter start offset date: \")\n",
    "    end_date_str = input(\"Enter end offset date: \")\n",
    "\n",
    "    # --- Execution Flow ---\n",
    "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "    \n",
    "    df_generated = generate_synthetic_data(start_date, end_date, num_records_per_day)\n",
    "\n",
    "    if not df_generated.empty:\n",
    "        df_transformed = transform_data(df_generated, start_date_str, end_date_str)\n",
    "        if df_transformed is not None:\n",
    "            load_data_to_mysql(df_transformed, db_config, table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
